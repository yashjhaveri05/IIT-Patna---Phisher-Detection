{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Riwalk.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzgWXgKi9NCe",
        "outputId": "b4367270-5af9-4a55-8d38-4932d769b052"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import pandas as pd\n",
        "from scipy.stats import entropy\n",
        "\n",
        "class NodeEngFeatures:\n",
        "    def __init__(self, nodes, edges):\n",
        "        self.nodes = nodes  # a dataframe\n",
        "        self.edges = edges  # a dataframe\n",
        "        self.G = nx.from_pandas_edgelist(self.edges, source='source', target='target',\n",
        "                                         edge_attr=['timestamp', 'amount'],\n",
        "                                         create_using=nx.MultiDiGraph())\n",
        "        print(\"*** Original MD-Graph ***\")\n",
        "        print(nx.info(self.G))\n",
        "        self.node_feature_names = self.retrieve_feature_name()\n",
        "\n",
        "    def retrieve_feature_name(self):\n",
        "        \"\"\"\n",
        "        retrieve the names of the features for the nodes\n",
        "        \"\"\"\n",
        "        feature_stat_df = FeatureStatus().feature_stat\n",
        "        feature_name_list = feature_stat_df['feature'].tolist()\n",
        "        return feature_name_list\n",
        "\n",
        "    def get_tx_amount_and_interval_list(self, node, opt):\n",
        "        \"\"\"\n",
        "        returns the list of amount and the list of timestamps for all the (opt-) transactions\n",
        "        :param node: the node that we focus on\n",
        "        :param opt: 'in', 'out', or 'all' transactions\n",
        "        \"\"\"\n",
        "        if opt == 'in':  # incoming tx\n",
        "            node_tx_df = self.edges[self.edges['target'] == node]\n",
        "        elif opt == 'out':  # outgoing tx\n",
        "            node_tx_df = self.edges[self.edges['source'] == node]\n",
        "        elif opt == 'all':  # all tx\n",
        "            node_tx_df = self.edges[(self.edges['target'] == node) | (self.edges['source'] == node)]\n",
        "        else:\n",
        "            raise ValueError(\"Option unavailable!\")\n",
        "\n",
        "        amount_list = node_tx_df['amount'].tolist()\n",
        "        linux_timestamp_list = node_tx_df['timestamp'].tolist()\n",
        "        timestamp_list = [datetime.datetime.fromtimestamp(t) for t in linux_timestamp_list]\n",
        "        timestamp_list.sort()\n",
        "        # interval of txs in minutes\n",
        "        tx_interval = [((timestamp_list[i + 1] - timestamp_list[i]).total_seconds() / 60) for i\n",
        "                       in range(len(timestamp_list) - 1)]\n",
        "\n",
        "        return amount_list, tx_interval\n",
        "\n",
        "    def neighbor_degree_features(self, node):\n",
        "        \"\"\"\n",
        "        get the features related to the degree distributions of the neighbors of the node\n",
        "        \"\"\"\n",
        "        # extract the egonet of the node\n",
        "        egonet = nx.ego_graph(self.G, node)\n",
        "\n",
        "        # prerequisite for some neighborhood features\n",
        "        egonet_node = nx.nodes(egonet)\n",
        "        no_edge_egonet_in = 0  # number of in-coming edges to egonet\n",
        "        no_edge_egonet_out = 0  # number of out-going edges from egonet\n",
        "        for nb_node in egonet_node:\n",
        "            if node != nb_node:\n",
        "                no_edge_egonet_in += (self.G.in_degree[nb_node] - egonet.in_degree[nb_node])\n",
        "                no_edge_egonet_out += (self.G.out_degree[nb_node] - egonet.out_degree[nb_node])\n",
        "\n",
        "        neighbor_degrees = [d for n, d in egonet.degree() if n != node]\n",
        "        neighbor_w_degrees = [d for n, d in egonet.degree(weight='amount') if n != node]\n",
        "        neighbor_in_degrees = [d for n, d in egonet.in_degree() if n != node]\n",
        "        neighbor_out_degrees = [d for n, d in egonet.out_degree() if n != node]\n",
        "\n",
        "        no_edge_egonet = egonet.number_of_edges()\n",
        "\n",
        "        return no_edge_egonet, no_edge_egonet_in, no_edge_egonet_out, \\\n",
        "               neighbor_degrees, neighbor_w_degrees, neighbor_in_degrees, neighbor_out_degrees\n",
        "\n",
        "    def gen_node_features_single(self, node):\n",
        "        \"\"\"\n",
        "        generate the features for the node\n",
        "        :param node: node of interest\n",
        "        \"\"\"\n",
        "        no_edge_egonet, no_edge_egonet_in, no_edge_egonet_out, \\\n",
        "            neighbor_degrees, neighbor_w_degrees, neighbor_in_degrees, neighbor_out_degrees = \\\n",
        "            self.neighbor_degree_features(node)\n",
        "        amnt_in_list, interval_in_tx = self.get_tx_amount_and_interval_list(node, 'in')\n",
        "        amnt_out_list, interval_out_tx = self.get_tx_amount_and_interval_list(node, 'out')\n",
        "        amnt_all_list, interval_all_tx = self.get_tx_amount_and_interval_list(node, 'all')\n",
        "        node_row = self.nodes.loc[self.nodes['node'] == node]\n",
        "        node_feature_dict = {\n",
        "            'node': node_row['node'].values[0],\n",
        "            # 'address': node_row['address'].values[0],\n",
        "            'isp': node_row['isp'].values[0],\n",
        "            # 'is_anchor': node_row['is_anchor'].values[0],\n",
        "            # 'balance': node_row['balance'].values[0],\n",
        "\n",
        "            # structural\n",
        "            'degree': len(amnt_all_list),\n",
        "            # 'w_degree': self.G.degree(node, weight='amount'),\n",
        "            'in_degree': len(amnt_in_list),\n",
        "            'out_degree': len(amnt_out_list),\n",
        "\n",
        "            # transactional\n",
        "            'avg_amount_in_tx': np.mean(amnt_in_list) if len(amnt_in_list) > 0 else 0,\n",
        "            'min_amount_in_tx': np.min(amnt_in_list) if len(amnt_in_list) > 0 else 0,\n",
        "            'max_amount_in_tx': np.max(amnt_in_list) if len(amnt_in_list) > 0 else 0,\n",
        "            'sum_amount_in_tx': np.sum(amnt_in_list),\n",
        "            'std_amount_in_tx': np.std(amnt_in_list) if len(amnt_in_list) > 0 else 0,\n",
        "            'ent_amount_in_tx': entropy(amnt_in_list) if np.sum(amnt_in_list) != 0 else 0,\n",
        "\n",
        "            'avg_in_tx_interval': np.mean(interval_in_tx) if len(interval_in_tx) > 0 else 0,\n",
        "            'min_in_tx_interval': np.min(interval_in_tx) if len(interval_in_tx) > 0 else 0,\n",
        "            'max_in_tx_interval': np.max(interval_in_tx) if len(interval_in_tx) > 0 else 0,\n",
        "            'sum_in_tx_interval': np.sum(interval_in_tx),\n",
        "            'std_in_tx_interval': np.std(interval_in_tx) if len(interval_in_tx) > 0 else 0,\n",
        "            'ent_in_tx_interval': entropy(interval_in_tx) if np.sum(interval_in_tx) != 0 else 0,\n",
        "\n",
        "            'avg_amount_out_tx': np.mean(amnt_out_list) if len(amnt_out_list) > 0 else 0,\n",
        "            'min_amount_out_tx': np.min(amnt_out_list) if len(amnt_out_list) > 0 else 0,\n",
        "            'max_amount_out_tx': np.max(amnt_out_list) if len(amnt_out_list) > 0 else 0,\n",
        "            'sum_amount_out_tx': np.sum(amnt_out_list),\n",
        "            'std_amount_out_tx': np.std(amnt_out_list) if len(amnt_out_list) > 0 else 0,\n",
        "            'ent_amount_out_tx': entropy(amnt_out_list) if np.sum(amnt_out_list) != 0 else 0,\n",
        "\n",
        "            'avg_out_tx_interval': np.mean(interval_out_tx) if len(interval_out_tx) > 0 else 0,\n",
        "            'min_out_tx_interval': np.min(interval_out_tx) if len(interval_out_tx) > 0 else 0,\n",
        "            'max_out_tx_interval': np.max(interval_out_tx) if len(interval_out_tx) > 0 else 0,\n",
        "            'sum_out_tx_interval': np.sum(interval_out_tx),\n",
        "            'std_out_tx_interval': np.std(interval_out_tx) if len(interval_out_tx) > 0 else 0,\n",
        "            'ent_out_tx_interval': entropy(interval_out_tx) if np.sum(interval_out_tx) != 0 else 0,\n",
        "\n",
        "            'avg_amount_all_tx': np.mean(amnt_all_list) if len(amnt_all_list) > 0 else 0,  # all tx: in & out\n",
        "            'min_amount_all_tx': np.min(amnt_all_list) if len(amnt_all_list) > 0 else 0,\n",
        "            'max_amount_all_tx': np.max(amnt_all_list) if len(amnt_all_list) > 0 else 0,\n",
        "            'sum_amount_all_tx': np.sum(amnt_all_list),  # this should be equal to weighted degree\n",
        "            'std_amount_all_tx': np.std(amnt_all_list) if len(amnt_all_list) > 0 else 0,\n",
        "            'ent_amount_all_tx': entropy(amnt_all_list) if np.sum(amnt_all_list) != 0 else 0,\n",
        "\n",
        "            'avg_all_tx_interval': np.mean(interval_all_tx) if len(interval_all_tx) > 0 else 0,\n",
        "            'min_all_tx_interval': np.min(interval_all_tx) if len(interval_all_tx) > 0 else 0,\n",
        "            'max_all_tx_interval': np.max(interval_all_tx) if len(interval_all_tx) > 0 else 0,\n",
        "            'sum_all_tx_interval': np.sum(interval_all_tx),\n",
        "            'std_all_tx_interval': np.std(interval_all_tx) if len(interval_all_tx) > 0 else 0,\n",
        "            'ent_all_tx_interval': entropy(interval_all_tx) if np.sum(interval_all_tx) != 0 else 0,\n",
        "\n",
        "            # regional features\n",
        "            'no_edge_within_egonet': no_edge_egonet,  # number of edges within the egonet for all nodes\n",
        "            'no_edge_in_egonet': no_edge_egonet_in,  # number of in-edges to the egonet\n",
        "            'no_edge_out_egonet': no_edge_egonet_out,  # number of out-edges from the egonet\n",
        "            'no_edge_all_egonet': no_edge_egonet_in + no_edge_egonet_out,  # total number of edges to/from the egonet\n",
        "\n",
        "            # neighborhood features\n",
        "            'avg_neighbor_degree': np.mean(neighbor_degrees) if len(neighbor_degrees) > 0 else 0,\n",
        "            'min_neighbor_degree': np.min(neighbor_degrees) if len(neighbor_degrees) > 0 else 0,\n",
        "            'max_neighbor_degree': np.max(neighbor_degrees) if len(neighbor_degrees) > 0 else 0,\n",
        "            'sum_neighbor_degree': np.sum(neighbor_degrees),\n",
        "            'std_neighbor_degree': np.std(neighbor_degrees) if len(neighbor_degrees) > 0 else 0,\n",
        "            'ent_neighbor_degree': entropy(neighbor_degrees) if np.sum(neighbor_degrees) != 0 else 0,\n",
        "\n",
        "            'avg_neighbor_w_degree': np.mean(neighbor_w_degrees) if len(neighbor_w_degrees) > 0 else 0,\n",
        "            'min_neighbor_w_degree': np.min(neighbor_w_degrees) if len(neighbor_w_degrees) > 0 else 0,\n",
        "            'max_neighbor_w_degree': np.max(neighbor_w_degrees) if len(neighbor_w_degrees) > 0 else 0,\n",
        "            'sum_neighbor_w_degree': np.sum(neighbor_w_degrees),\n",
        "            'std_neighbor_w_degree': np.std(neighbor_w_degrees) if len(neighbor_w_degrees) > 0 else 0,\n",
        "            'ent_neighbor_w_degree': entropy(neighbor_w_degrees) if np.sum(neighbor_w_degrees) != 0 else 0,\n",
        "\n",
        "            'avg_neighbor_in_degree': np.mean(neighbor_in_degrees) if len(neighbor_in_degrees) > 0 else 0,\n",
        "            'min_neighbor_in_degree': np.min(neighbor_in_degrees) if len(neighbor_in_degrees) > 0 else 0,\n",
        "            'max_neighbor_in_degree': np.max(neighbor_in_degrees) if len(neighbor_in_degrees) > 0 else 0,\n",
        "            'sum_neighbor_in_degree': np.sum(neighbor_in_degrees),\n",
        "            'std_neighbor_in_degree': np.std(neighbor_in_degrees) if len(neighbor_in_degrees) > 0 else 0,\n",
        "            'ent_neighbor_in_degree': entropy(neighbor_in_degrees) if np.sum(neighbor_in_degrees) != 0 else 0,\n",
        "\n",
        "            'avg_neighbor_out_degree': np.mean(neighbor_out_degrees) if len(neighbor_out_degrees) > 0 else 0,\n",
        "            'min_neighbor_out_degree': np.min(neighbor_out_degrees) if len(neighbor_out_degrees) > 0 else 0,\n",
        "            'max_neighbor_out_degree': np.max(neighbor_out_degrees) if len(neighbor_out_degrees) > 0 else 0,\n",
        "            'sum_neighbor_out_degree': np.sum(neighbor_out_degrees),\n",
        "            'std_neighbor_out_degree': np.std(neighbor_out_degrees) if len(neighbor_out_degrees) > 0 else 0,\n",
        "            'ent_neighbor_out_degree': entropy(neighbor_out_degrees) if np.sum(neighbor_out_degrees) != 0 else 0,\n",
        "\n",
        "        }\n",
        "        return node_feature_dict\n",
        "\n",
        "    def gen_node_features_list(self, node_list):\n",
        "        \"\"\"\n",
        "        generate features for each node in the node_list\n",
        "        :param node_list: a list of different nodes\n",
        "        :return node_feature_df: a dataframe of the nodes and their features\n",
        "        \"\"\"\n",
        "        node_features_dict_list = [self.gen_node_features_single(node) for node in node_list]\n",
        "        node_feature_df = pd.DataFrame(node_features_dict_list, columns=self.node_feature_names)\n",
        "        return node_feature_df"
      ],
      "metadata": {
        "id": "HR6MqZ8Z9ZS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import pandas as pd\n",
        "from scipy.stats import entropy\n",
        "\n",
        "class NodeEngFeatures:\n",
        "    def __init__(self, nodes, edges):\n",
        "        self.nodes = nodes  # a dataframe\n",
        "        self.edges = edges  # a dataframe\n",
        "        self.G = nx.from_pandas_edgelist(self.edges, source='source', target='target',\n",
        "                                         edge_attr=['timestamp', 'amount'],\n",
        "                                         create_using=nx.MultiDiGraph())\n",
        "        print(\"*** Original MD-Graph ***\")\n",
        "        print(nx.info(self.G))\n",
        "        self.node_feature_names = self.retrieve_feature_name()\n",
        "\n",
        "    def retrieve_feature_name(self):\n",
        "        \"\"\"\n",
        "        retrieve the names of the features for the nodes\n",
        "        \"\"\"\n",
        "        feature_stat_df = FeatureStatus().feature_stat\n",
        "        feature_name_list = feature_stat_df['feature'].tolist()\n",
        "        return feature_name_list\n",
        "\n",
        "    def get_tx_amount_and_interval_list(self, node, opt):\n",
        "        \"\"\"\n",
        "        returns the list of amount and the list of timestamps for all the (opt-) transactions\n",
        "        :param node: the node that we focus on\n",
        "        :param opt: 'in', 'out', or 'all' transactions\n",
        "        \"\"\"\n",
        "        if opt == 'in':  # incoming tx\n",
        "            node_tx_df = self.edges[self.edges['target'] == node]\n",
        "        elif opt == 'out':  # outgoing tx\n",
        "            node_tx_df = self.edges[self.edges['source'] == node]\n",
        "        elif opt == 'all':  # all tx\n",
        "            node_tx_df = self.edges[(self.edges['target'] == node) | (self.edges['source'] == node)]\n",
        "        else:\n",
        "            raise ValueError(\"Option unavailable!\")\n",
        "\n",
        "        amount_list = node_tx_df['amount'].tolist()\n",
        "        linux_timestamp_list = node_tx_df['timestamp'].tolist()\n",
        "        timestamp_list = [datetime.datetime.fromtimestamp(t) for t in linux_timestamp_list]\n",
        "        timestamp_list.sort()\n",
        "        # interval of txs in minutes\n",
        "        tx_interval = [((timestamp_list[i + 1] - timestamp_list[i]).total_seconds() / 60) for i\n",
        "                       in range(len(timestamp_list) - 1)]\n",
        "\n",
        "        return amount_list, tx_interval\n",
        "\n",
        "    def neighbor_degree_features(self, node):\n",
        "        \"\"\"\n",
        "        get the features related to the degree distributions of the neighbors of the node\n",
        "        \"\"\"\n",
        "        # extract the egonet of the node\n",
        "        egonet = nx.ego_graph(self.G, node)\n",
        "\n",
        "        # prerequisite for some neighborhood features\n",
        "        egonet_node = nx.nodes(egonet)\n",
        "        no_edge_egonet_in = 0  # number of in-coming edges to egonet\n",
        "        no_edge_egonet_out = 0  # number of out-going edges from egonet\n",
        "        for nb_node in egonet_node:\n",
        "            if node != nb_node:\n",
        "                no_edge_egonet_in += (self.G.in_degree[nb_node] - egonet.in_degree[nb_node])\n",
        "                no_edge_egonet_out += (self.G.out_degree[nb_node] - egonet.out_degree[nb_node])\n",
        "\n",
        "        neighbor_degrees = [d for n, d in egonet.degree() if n != node]\n",
        "        neighbor_w_degrees = [d for n, d in egonet.degree(weight='amount') if n != node]\n",
        "        neighbor_in_degrees = [d for n, d in egonet.in_degree() if n != node]\n",
        "        neighbor_out_degrees = [d for n, d in egonet.out_degree() if n != node]\n",
        "\n",
        "        no_edge_egonet = egonet.number_of_edges()\n",
        "\n",
        "        return no_edge_egonet, no_edge_egonet_in, no_edge_egonet_out, \\\n",
        "               neighbor_degrees, neighbor_w_degrees, neighbor_in_degrees, neighbor_out_degrees\n",
        "\n",
        "    def gen_node_features_single(self, node):\n",
        "        \"\"\"\n",
        "        generate the features for the node\n",
        "        :param node: node of interest\n",
        "        \"\"\"\n",
        "        no_edge_egonet, no_edge_egonet_in, no_edge_egonet_out, \\\n",
        "            neighbor_degrees, neighbor_w_degrees, neighbor_in_degrees, neighbor_out_degrees = \\\n",
        "            self.neighbor_degree_features(node)\n",
        "        amnt_in_list, interval_in_tx = self.get_tx_amount_and_interval_list(node, 'in')\n",
        "        amnt_out_list, interval_out_tx = self.get_tx_amount_and_interval_list(node, 'out')\n",
        "        amnt_all_list, interval_all_tx = self.get_tx_amount_and_interval_list(node, 'all')\n",
        "        node_row = self.nodes.loc[self.nodes['node'] == node]\n",
        "        node_feature_dict = {\n",
        "            'node': node_row['node'].values[0],\n",
        "            # 'address': node_row['address'].values[0],\n",
        "            'isp': node_row['isp'].values[0],\n",
        "            # 'is_anchor': node_row['is_anchor'].values[0],\n",
        "            # 'balance': node_row['balance'].values[0],\n",
        "\n",
        "            # structural\n",
        "            'degree': len(amnt_all_list),\n",
        "            # 'w_degree': self.G.degree(node, weight='amount'),\n",
        "            'in_degree': len(amnt_in_list),\n",
        "            'out_degree': len(amnt_out_list),\n",
        "\n",
        "            # transactional\n",
        "            'avg_amount_in_tx': np.mean(amnt_in_list) if len(amnt_in_list) > 0 else 0,\n",
        "            'min_amount_in_tx': np.min(amnt_in_list) if len(amnt_in_list) > 0 else 0,\n",
        "            'max_amount_in_tx': np.max(amnt_in_list) if len(amnt_in_list) > 0 else 0,\n",
        "            'sum_amount_in_tx': np.sum(amnt_in_list),\n",
        "            'std_amount_in_tx': np.std(amnt_in_list) if len(amnt_in_list) > 0 else 0,\n",
        "            'ent_amount_in_tx': entropy(amnt_in_list) if np.sum(amnt_in_list) != 0 else 0,\n",
        "\n",
        "            'avg_in_tx_interval': np.mean(interval_in_tx) if len(interval_in_tx) > 0 else 0,\n",
        "            'min_in_tx_interval': np.min(interval_in_tx) if len(interval_in_tx) > 0 else 0,\n",
        "            'max_in_tx_interval': np.max(interval_in_tx) if len(interval_in_tx) > 0 else 0,\n",
        "            'sum_in_tx_interval': np.sum(interval_in_tx),\n",
        "            'std_in_tx_interval': np.std(interval_in_tx) if len(interval_in_tx) > 0 else 0,\n",
        "            'ent_in_tx_interval': entropy(interval_in_tx) if np.sum(interval_in_tx) != 0 else 0,\n",
        "\n",
        "            'avg_amount_out_tx': np.mean(amnt_out_list) if len(amnt_out_list) > 0 else 0,\n",
        "            'min_amount_out_tx': np.min(amnt_out_list) if len(amnt_out_list) > 0 else 0,\n",
        "            'max_amount_out_tx': np.max(amnt_out_list) if len(amnt_out_list) > 0 else 0,\n",
        "            'sum_amount_out_tx': np.sum(amnt_out_list),\n",
        "            'std_amount_out_tx': np.std(amnt_out_list) if len(amnt_out_list) > 0 else 0,\n",
        "            'ent_amount_out_tx': entropy(amnt_out_list) if np.sum(amnt_out_list) != 0 else 0,\n",
        "\n",
        "            'avg_out_tx_interval': np.mean(interval_out_tx) if len(interval_out_tx) > 0 else 0,\n",
        "            'min_out_tx_interval': np.min(interval_out_tx) if len(interval_out_tx) > 0 else 0,\n",
        "            'max_out_tx_interval': np.max(interval_out_tx) if len(interval_out_tx) > 0 else 0,\n",
        "            'sum_out_tx_interval': np.sum(interval_out_tx),\n",
        "            'std_out_tx_interval': np.std(interval_out_tx) if len(interval_out_tx) > 0 else 0,\n",
        "            'ent_out_tx_interval': entropy(interval_out_tx) if np.sum(interval_out_tx) != 0 else 0,\n",
        "\n",
        "            'avg_amount_all_tx': np.mean(amnt_all_list) if len(amnt_all_list) > 0 else 0,  # all tx: in & out\n",
        "            'min_amount_all_tx': np.min(amnt_all_list) if len(amnt_all_list) > 0 else 0,\n",
        "            'max_amount_all_tx': np.max(amnt_all_list) if len(amnt_all_list) > 0 else 0,\n",
        "            'sum_amount_all_tx': np.sum(amnt_all_list),  # this should be equal to weighted degree\n",
        "            'std_amount_all_tx': np.std(amnt_all_list) if len(amnt_all_list) > 0 else 0,\n",
        "            'ent_amount_all_tx': entropy(amnt_all_list) if np.sum(amnt_all_list) != 0 else 0,\n",
        "\n",
        "            'avg_all_tx_interval': np.mean(interval_all_tx) if len(interval_all_tx) > 0 else 0,\n",
        "            'min_all_tx_interval': np.min(interval_all_tx) if len(interval_all_tx) > 0 else 0,\n",
        "            'max_all_tx_interval': np.max(interval_all_tx) if len(interval_all_tx) > 0 else 0,\n",
        "            'sum_all_tx_interval': np.sum(interval_all_tx),\n",
        "            'std_all_tx_interval': np.std(interval_all_tx) if len(interval_all_tx) > 0 else 0,\n",
        "            'ent_all_tx_interval': entropy(interval_all_tx) if np.sum(interval_all_tx) != 0 else 0,\n",
        "\n",
        "            # regional features\n",
        "            'no_edge_within_egonet': no_edge_egonet,  # number of edges within the egonet for all nodes\n",
        "            'no_edge_in_egonet': no_edge_egonet_in,  # number of in-edges to the egonet\n",
        "            'no_edge_out_egonet': no_edge_egonet_out,  # number of out-edges from the egonet\n",
        "            'no_edge_all_egonet': no_edge_egonet_in + no_edge_egonet_out,  # total number of edges to/from the egonet\n",
        "\n",
        "            # neighborhood features\n",
        "            'avg_neighbor_degree': np.mean(neighbor_degrees) if len(neighbor_degrees) > 0 else 0,\n",
        "            'min_neighbor_degree': np.min(neighbor_degrees) if len(neighbor_degrees) > 0 else 0,\n",
        "            'max_neighbor_degree': np.max(neighbor_degrees) if len(neighbor_degrees) > 0 else 0,\n",
        "            'sum_neighbor_degree': np.sum(neighbor_degrees),\n",
        "            'std_neighbor_degree': np.std(neighbor_degrees) if len(neighbor_degrees) > 0 else 0,\n",
        "            'ent_neighbor_degree': entropy(neighbor_degrees) if np.sum(neighbor_degrees) != 0 else 0,\n",
        "\n",
        "            'avg_neighbor_w_degree': np.mean(neighbor_w_degrees) if len(neighbor_w_degrees) > 0 else 0,\n",
        "            'min_neighbor_w_degree': np.min(neighbor_w_degrees) if len(neighbor_w_degrees) > 0 else 0,\n",
        "            'max_neighbor_w_degree': np.max(neighbor_w_degrees) if len(neighbor_w_degrees) > 0 else 0,\n",
        "            'sum_neighbor_w_degree': np.sum(neighbor_w_degrees),\n",
        "            'std_neighbor_w_degree': np.std(neighbor_w_degrees) if len(neighbor_w_degrees) > 0 else 0,\n",
        "            'ent_neighbor_w_degree': entropy(neighbor_w_degrees) if np.sum(neighbor_w_degrees) != 0 else 0,\n",
        "\n",
        "            'avg_neighbor_in_degree': np.mean(neighbor_in_degrees) if len(neighbor_in_degrees) > 0 else 0,\n",
        "            'min_neighbor_in_degree': np.min(neighbor_in_degrees) if len(neighbor_in_degrees) > 0 else 0,\n",
        "            'max_neighbor_in_degree': np.max(neighbor_in_degrees) if len(neighbor_in_degrees) > 0 else 0,\n",
        "            'sum_neighbor_in_degree': np.sum(neighbor_in_degrees),\n",
        "            'std_neighbor_in_degree': np.std(neighbor_in_degrees) if len(neighbor_in_degrees) > 0 else 0,\n",
        "            'ent_neighbor_in_degree': entropy(neighbor_in_degrees) if np.sum(neighbor_in_degrees) != 0 else 0,\n",
        "\n",
        "            'avg_neighbor_out_degree': np.mean(neighbor_out_degrees) if len(neighbor_out_degrees) > 0 else 0,\n",
        "            'min_neighbor_out_degree': np.min(neighbor_out_degrees) if len(neighbor_out_degrees) > 0 else 0,\n",
        "            'max_neighbor_out_degree': np.max(neighbor_out_degrees) if len(neighbor_out_degrees) > 0 else 0,\n",
        "            'sum_neighbor_out_degree': np.sum(neighbor_out_degrees),\n",
        "            'std_neighbor_out_degree': np.std(neighbor_out_degrees) if len(neighbor_out_degrees) > 0 else 0,\n",
        "            'ent_neighbor_out_degree': entropy(neighbor_out_degrees) if np.sum(neighbor_out_degrees) != 0 else 0,\n",
        "\n",
        "        }\n",
        "        return node_feature_dict\n",
        "\n",
        "    def gen_node_features_list(self, node_list):\n",
        "        \"\"\"\n",
        "        generate features for each node in the node_list\n",
        "        :param node_list: a list of different nodes\n",
        "        :return node_feature_df: a dataframe of the nodes and their features\n",
        "        \"\"\"\n",
        "        node_features_dict_list = [self.gen_node_features_single(node) for node in node_list]\n",
        "        node_feature_df = pd.DataFrame(node_features_dict_list, columns=self.node_feature_names)\n",
        "        return node_feature_df"
      ],
      "metadata": {
        "id": "ZM5J934l9asp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "class FeatureStatus:\n",
        "    def __init__(self):\n",
        "        self.feature_stat = self.generate_feature_status()\n",
        "\n",
        "    def generate_feature_status(self):\n",
        "        \"\"\"\"\n",
        "        generate the status of different features\n",
        "        \"\"\"\n",
        "        feature_df_column_name = ['feature', 'select']\n",
        "        feature_dict = {\n",
        "            # major features\n",
        "            0: ['node', 1],\n",
        "\n",
        "            # 1: ['address', 0],  # NEVER select\n",
        "            2: ['isp', 0],  # NEVER select\n",
        "            # 3: ['is_anchor', 0],  # NEVER select\n",
        "\n",
        "            5: ['degree', 0],\n",
        "            6: ['in_degree', 1],  # this\n",
        "            7: ['out_degree', 0],\n",
        "\n",
        "            8: ['avg_amount_in_tx', 0],\n",
        "            9: ['min_amount_in_tx', 0],\n",
        "            10: ['max_amount_in_tx', 0],\n",
        "            11: ['sum_amount_in_tx', 1],  # this\n",
        "            12: ['std_amount_in_tx', 1],  # this\n",
        "            13: ['ent_amount_in_tx', 0],\n",
        "\n",
        "            14: ['avg_in_tx_interval', 1],  # this\n",
        "            15: ['min_in_tx_interval', 0],\n",
        "            16: ['max_in_tx_interval', 1],  # this\n",
        "            17: ['sum_in_tx_interval', 1],  # this\n",
        "            18: ['std_in_tx_interval', 0],\n",
        "            19: ['ent_in_tx_interval', 0],\n",
        "\n",
        "            20: ['avg_amount_out_tx', 0],\n",
        "            21: ['min_amount_out_tx', 0],\n",
        "            22: ['max_amount_out_tx', 0],\n",
        "            23: ['sum_amount_out_tx', 1],  # this\n",
        "            24: ['std_amount_out_tx', 0],\n",
        "            25: ['ent_amount_out_tx', 0],\n",
        "\n",
        "            26: ['avg_out_tx_interval', 0],\n",
        "            27: ['min_out_tx_interval', 0],\n",
        "            28: ['max_out_tx_interval', 0],\n",
        "            29: ['sum_out_tx_interval', 0],\n",
        "            30: ['std_out_tx_interval', 0],\n",
        "            31: ['ent_out_tx_interval', 0],\n",
        "\n",
        "            32: ['avg_amount_all_tx', 0],\n",
        "            33: ['min_amount_all_tx', 0],\n",
        "            34: ['max_amount_all_tx', 0],\n",
        "            35: ['sum_amount_all_tx', 0],\n",
        "            36: ['std_amount_all_tx', 0],\n",
        "            37: ['ent_amount_all_tx', 1],  # this\n",
        "\n",
        "            38: ['avg_all_tx_interval', 0],\n",
        "            39: ['min_all_tx_interval', 0],\n",
        "            40: ['max_all_tx_interval', 0],\n",
        "            41: ['sum_all_tx_interval', 0],\n",
        "            42: ['std_all_tx_interval', 0],\n",
        "            43: ['ent_all_tx_interval', 0],\n",
        "\n",
        "            44: ['no_edge_within_egonet', 0],\n",
        "            45: ['no_edge_in_egonet', 1],  # this\n",
        "            46: ['no_edge_out_egonet', 0],\n",
        "            47: ['no_edge_all_egonet', 1],  # this\n",
        "\n",
        "            48: ['avg_neighbor_degree', 0],\n",
        "            49: ['min_neighbor_degree', 0],\n",
        "            50: ['max_neighbor_degree', 0],\n",
        "            51: ['sum_neighbor_degree', 0],\n",
        "            52: ['std_neighbor_degree', 0],\n",
        "            53: ['ent_neighbor_degree', 0],\n",
        "\n",
        "            54: ['avg_neighbor_w_degree', 0],\n",
        "            55: ['min_neighbor_w_degree', 0],\n",
        "            56: ['max_neighbor_w_degree', 0],\n",
        "            57: ['sum_neighbor_w_degree', 0],\n",
        "            58: ['std_neighbor_w_degree', 0],\n",
        "            59: ['ent_neighbor_w_degree', 0],\n",
        "\n",
        "            60: ['avg_neighbor_in_degree', 0],\n",
        "            61: ['min_neighbor_in_degree', 0],\n",
        "            62: ['max_neighbor_in_degree', 0],\n",
        "            63: ['sum_neighbor_in_degree', 0],\n",
        "            64: ['std_neighbor_in_degree', 0],\n",
        "            65: ['ent_neighbor_in_degree', 0],\n",
        "\n",
        "            66: ['avg_neighbor_out_degree', 0],\n",
        "            67: ['min_neighbor_out_degree', 0],\n",
        "            68: ['max_neighbor_out_degree', 0],\n",
        "            69: ['sum_neighbor_out_degree', 0],\n",
        "            70: ['std_neighbor_out_degree', 0],\n",
        "            71: ['ent_neighbor_out_degree', 0],\n",
        "\n",
        "            # 62: ['balance', 1, 0],\n",
        "\n",
        "            # derived features\n",
        "        }\n",
        "\n",
        "        feature_df = pd.DataFrame.from_dict(feature_dict, orient='index',\n",
        "                                            columns=feature_df_column_name)\n",
        "\n",
        "        return feature_df"
      ],
      "metadata": {
        "id": "UIVXur819pnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\"\n",
        "Preparation of the graph that is going to be used by RiWalk --- Ethereum\n",
        "\"\"\"\n",
        "\n",
        "# --- import statements ---\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import time\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "\n",
        "# --- variables ---\n",
        "rnd_seed = 42\n",
        "\n",
        "\n",
        "class NodeFeature:\n",
        "    \"\"\"\n",
        "    a class to define the appropriate features for ALL nodes of the graph\n",
        "    this facilitates the modifications to the RiWalk\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, nodes, edges):\n",
        "        self.nodes = nodes\n",
        "        self.edges = edges\n",
        "        self.n_fe_features = NodeEngFeatures(self.nodes, self.edges)\n",
        "\n",
        "    def generate_edge_list_for_RiWalk_unweighted(self):\n",
        "        \"\"\"\n",
        "        generates graphs for the original RiWalk method\n",
        "        :return unweighted, directed, simple graph\n",
        "        \"\"\"\n",
        "        grouped_edges = self.edges.groupby(['source', 'target'])\n",
        "        source_list = []\n",
        "        target_list = []\n",
        "        for key, value in grouped_edges:\n",
        "            source_list.append(key[0])  # source node\n",
        "            target_list.append(key[1])  # target node\n",
        "        simp_dir_edge_list = pd.DataFrame(list(zip(source_list, target_list)), columns=['source', 'target'])\n",
        "\n",
        "        return simp_dir_edge_list\n",
        "\n",
        "    def generate_edge_list_for_RiWalk_weighted(self):\n",
        "        \"\"\"\n",
        "        generate graph for RiWalk\n",
        "        convert MDG to a simple directed weighted graph\n",
        "        \"\"\"\n",
        "        mdg = nx.from_pandas_edgelist(self.edges, source='source', target='target',\n",
        "                                      edge_attr=['amount', 'timestamp'], create_using=nx.MultiDiGraph())\n",
        "\n",
        "        # generate simple graph\n",
        "        simG = nx.DiGraph()\n",
        "        for u, v, data in mdg.edges(data=True):\n",
        "            a = data['amount'] if 'amount' in data else 0\n",
        "            t = data['timestamp'] if 'timestamp' in data else 0\n",
        "\n",
        "            if simG.has_edge(u, v):\n",
        "                simG[u][v]['amount'] += a  # sum of amounts\n",
        "\n",
        "                current_timestamp = simG[u][v]['timestamp']\n",
        "                simG[u][v]['timestamp'] = max(t, current_timestamp)  # more recent\n",
        "\n",
        "                simG[u][v]['n_tx'] += 1\n",
        "            else:\n",
        "                simG.add_edge(u, v, amount=a, timestamp=t, n_tx=1)\n",
        "\n",
        "        # normalize weight values\n",
        "        for u, v, data in simG.edges(data=True):\n",
        "            if simG.degree(u, weight='amount') != 0:\n",
        "                simG[u][v]['amount'] = simG[u][v]['amount'] / simG.degree(u, weight='amount')\n",
        "            else:\n",
        "                simG[u][v]['amount'] = 0\n",
        "            simG[u][v]['timestamp'] = simG[u][v]['timestamp'] / simG.degree(u, weight='timestamp')\n",
        "            simG[u][v]['n_tx'] = simG[u][v]['n_tx'] / simG.degree(u)\n",
        "\n",
        "            # aggregated weight value\n",
        "            simG[u][v]['weight'] = simG[u][v]['amount'] * simG[u][v]['timestamp'] * simG[u][v]['n_tx']\n",
        "\n",
        "        edge_list_df = nx.to_pandas_edgelist(simG, source='source', target='target')\n",
        "        # only preserve the 'weight'\n",
        "        edge_list_df = edge_list_df.drop(['amount', 'timestamp', 'n_tx'], axis=1)\n",
        "\n",
        "        return edge_list_df\n",
        "\n",
        "    def gen_features_all_nodes(self):\n",
        "        \"\"\"\n",
        "        get a dataframe containing the selected features for all nodes\n",
        "        \"\"\"\n",
        "        node_list = self.nodes['node'].tolist()  # select all the nodes\n",
        "        all_feature_df = self.n_fe_features.gen_node_features_list(node_list)\n",
        "        return all_feature_df\n",
        "\n",
        "    def gen_t_SNE_components_2d(self, features_df, shift_coff):\n",
        "        \"\"\"\n",
        "        generate the t-SNE components of a set of features\n",
        "        * the 2 first most important components\n",
        "        \"\"\"\n",
        "        values = features_df.drop(['node', 'isp'], axis=1).values.tolist()\n",
        "\n",
        "        min_max_scaler = MinMaxScaler()\n",
        "        values_scaled = min_max_scaler.fit_transform(values)\n",
        "\n",
        "        print('\\tGraphPrep; t-SNE starts.')\n",
        "        time_start = time.time()\n",
        "        tsne = TSNE(n_components=2, verbose=0, perplexity=40, n_iter=300)\n",
        "        tsne_results = tsne.fit_transform(values_scaled)\n",
        "        print('\\tt-SNE done! Time elapsed: {} seconds'.format(time.time() - time_start))\n",
        "\n",
        "        # plotting\n",
        "        min_first = [np.abs(np.min(tsne_results[:, 0])) * shift_coff] * len(tsne_results)\n",
        "        min_second = [np.abs(np.min(tsne_results[:, 1])) * shift_coff] * len(tsne_results)\n",
        "        tsne_data = {'tsne-2d-first': tsne_results[:, 0] + min_first,\n",
        "                     'tsne-2d-second': tsne_results[:, 1] + min_second,\n",
        "                     }\n",
        "        return tsne_data\n",
        "\n",
        "\n",
        "def generate_edgelist_for_RiWalk(n_feature, edgelist_RiWalk_filename):\n",
        "    \"\"\"\n",
        "    generates edge list for RiWalk\n",
        "    \"\"\"\n",
        "    # generate edge-list\n",
        "    print(\"\\tEdge-list generation starts.\")\n",
        "    start_time = time.time()\n",
        "    # simp_dir_edge_list = n_feature.generate_edge_list_for_RiWalk_unweighted()\n",
        "    simp_dir_edge_list = n_feature.generate_edge_list_for_RiWalk_weighted()\n",
        "    # simp_dir_edge_list.to_csv(edgelist_RiWalk_filename, sep=' ', index=False, header=False)\n",
        "    simp_dir_edge_list.to_csv(edgelist_RiWalk_filename, index=False)\n",
        "    print(\"\\tEdge-list generation lasted {} seconds.\".format(time.time() - start_time))\n",
        "\n",
        "\n",
        "def generate_features_for_all_nodes(n_feature, features_filename):\n",
        "    \"\"\"\n",
        "    generates node-list with features for RiWalk-NA\n",
        "    \"\"\"\n",
        "    # generate node-features-df\n",
        "    print(\"\\tFeatures generation starts.\")\n",
        "    start_time = time.time()\n",
        "    nodes_all_features_df = n_feature.gen_features_all_nodes()\n",
        "    nodes_all_features_df.to_csv(features_filename, index=False)\n",
        "    print(\"\\tFeature generation lasted {} seconds.\".format(time.time() - start_time))\n",
        "    return nodes_all_features_df\n",
        "\n",
        "\n",
        "def generate_node_list_for_RiWalk(nodes_all_features_df, selected_features,\n",
        "                                  n_feature, tsne, nodelist_RiWalk_filename):\n",
        "    print(\"\\tSelecting features for RiWalk node-list.\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    if not tsne:\n",
        "        # select some features\n",
        "        nodes_selected_features_df = nodes_all_features_df[selected_features].copy()\n",
        "    else:\n",
        "        # t-SNE components\n",
        "        shift_coeff = 10\n",
        "        tsne_data = n_feature.gen_t_SNE_components_2d(nodes_all_features_df, shift_coeff)\n",
        "        nodes_selected_features_df = pd.DataFrame(list(zip(nodes_all_features_df['node'].tolist(),\n",
        "                                                           tsne_data['tsne-2d-first'],\n",
        "                                                           tsne_data['tsne-2d-second'])),\n",
        "                                                  columns=['node', 'tsne_2d_1', 'tsne_2d_2'])\n",
        "\n",
        "    nodes_selected_features_df.to_csv(nodelist_RiWalk_filename, index=False)\n",
        "    print(\"\\tFeature selection lasted {} seconds.\".format(time.time() - start_time))\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Experiments\n",
        "    \"\"\"\n",
        "    gprep_opt = 'ri'\n",
        "\n",
        "  \n",
        "    # generated file\n",
        "    nodelist_RiWalk_filename = \"/content/drive/My Drive/Baseline/Dataset/nodelist.nodelist\"\n",
        "    edgelist_RiWalk_filename = \"/content/drive/My Drive/Baseline/Dataset/edgelist.edgelist\"\n",
        "    node_filename = \"/content/drive/My Drive/Baseline/Dataset/nodeData.csv\"\n",
        "    edge_filename = \"/content/drive/My Drive/Baseline/Dataset/edgeData.csv\"\n",
        "    features_filename = \"/content/drive/My Drive/Baseline/Dataset/features.csv\"\n",
        "    feat_imp_filename = \"/content/drive/My Drive/Baseline/Dataset/imp_features.csv\"\n",
        "\n",
        "    nodes = pd.read_csv(node_filename)\n",
        "    edges = pd.read_csv(edge_filename)\n",
        "\n",
        "\n",
        "    n_feature = NodeFeature(nodes, edges)\n",
        "\n",
        "    # graph pre-paration tasks\n",
        "    if gprep_opt == 'feature':\n",
        "        generate_features_for_all_nodes(n_feature, features_filename)\n",
        "    elif gprep_opt == 'ri':\n",
        "        # generate edge list for RiWalk\n",
        "        generate_edgelist_for_RiWalk(n_feature, edgelist_RiWalk_filename)\n",
        "        # generate node list for RiWalk\n",
        "        nodes_all_features_df = pd.read_csv(features_filename)\n",
        "        feature_rank = pd.read_csv(feat_imp_filename)['feature'].tolist()\n",
        "        selected_features = ['node'] + feature_rank[0:10]\n",
        "\n",
        "        tsne = False  # use TSNE components\n",
        "        generate_node_list_for_RiWalk(nodes_all_features_df, selected_features, n_feature,\n",
        "                                      tsne, nodelist_RiWalk_filename)\n",
        "    else:\n",
        "        raise ValueError(\"Incorrect value for graph preparation option!\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LcgxI9AX9c87",
        "outputId": "54ec6e71-1332-4891-c938-49208cb05258"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Original MD-Graph ***\n",
            "MultiDiGraph with 35417 nodes and 81000 edges\n",
            "\tEdge-list generation starts.\n",
            "\tEdge-list generation lasted 23.30220079421997 seconds.\n",
            "\tSelecting features for RiWalk node-list.\n",
            "\tFeature selection lasted 0.4857761859893799 seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Implement the node classification task\n",
        "In fact, this file is a collection of utility functions\n",
        "\"\"\"\n",
        "\n",
        "# --- import statements ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import random\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.metrics import f1_score, precision_recall_fscore_support, accuracy_score, roc_auc_score\n",
        "from sklearn.manifold import TSNE\n",
        "import time\n",
        "import seaborn as sns\n",
        "import math\n",
        "\n",
        "\n",
        "# --- parameters ---\n",
        "rnd_seed = 42\n",
        "random.seed(rnd_seed)\n",
        "test_size = 0.2\n",
        "\n",
        "\n",
        "# --- utility functions ---\n",
        "\n",
        "def perf_report(identifier, y_true, y_pred, binary, print_enable=False):\n",
        "    if binary:\n",
        "        # print(\">>> Binary Classification.\")\n",
        "        prec, rec, f1, num = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
        "        micro_f1 = f1_score(y_true, y_pred, average='binary')\n",
        "    else:\n",
        "        print(\">>> Multi-class Classification.\")\n",
        "        prec, rec, f1, num = precision_recall_fscore_support(y_true, y_pred, average='macro')\n",
        "        micro_f1 = f1_score(y_true, y_pred, average='macro')\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    if print_enable:\n",
        "        print(\"\\t*** {} performance reports: ***\".format(str(identifier)))\n",
        "        print(\"\\t\\tPrecision: %.3f \\n\\t\\tRecall: %.3f \\n\\t\\tF1-Score: %.3f\" % (prec, rec, f1))\n",
        "        print('\\t\\tMicro-Average F1-Score: %.3f' % micro_f1)\n",
        "        print('\\t\\tAccuracy: %.3f' % acc)\n",
        "        print(classification_report(y_true, y_pred))\n",
        "    return prec, rec, f1, acc\n",
        "\n",
        "\n",
        "def train_test_split(X, y, rnd_seed):\n",
        "    \"\"\"\n",
        "    split the features and the labels according to the indices\n",
        "    :param X: feature set, should be array or list\n",
        "    :param y: labels, should be array or list\n",
        "    :param rnd_seed: random seed\n",
        "    \"\"\"\n",
        "    # generate indices for the train and test set\n",
        "    indices = [i for i in range(len(y))]\n",
        "    sss = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=rnd_seed)\n",
        "    sss.get_n_splits(indices, y)\n",
        "    train_indices, test_indices = next(sss.split(indices, y))\n",
        "\n",
        "    # train/test split\n",
        "    X_train = [X[i] for i in train_indices]\n",
        "    X_test = [X[i] for i in test_indices]\n",
        "\n",
        "    y_train = [y[i] for i in train_indices]\n",
        "    y_test = [y[i] for i in test_indices]\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "\n",
        "def simple_classification(clf, clf_id, emb_flag, X_train, X_test, y_train, y_test,\n",
        "                          binary, exp_id, print_enable=False):\n",
        "    \"\"\"\n",
        "    train the model on the train set and test it on the test set.\n",
        "    to be consistent among different run, the indices are passed.\n",
        "    important NOTE: it is implicitly inferred that the positive label is 1.\n",
        "    no cross-validation is applied.\n",
        "    \"\"\"\n",
        "    print(\"C\")\n",
        "    print(X_train, y_train)\n",
        "    # train the model\n",
        "    clf.fit(X_train, y_train)\n",
        "    print(\"D\")\n",
        "    # predict the training set labels\n",
        "    y_train_pred = clf.predict(X_train)\n",
        "\n",
        "    # predict the test set labels\n",
        "    y_test_pred = clf.predict(X_test)\n",
        "\n",
        "    # evaluate the performance for the training set\n",
        "    tr_prec, tr_rec, tr_f1, tr_acc = perf_report(str(clf_id) + ' - Training Set',\n",
        "                                                 y_train, y_train_pred, binary, print_enable)\n",
        "    ts_prec, ts_rec, ts_f1, ts_acc = perf_report(str(clf_id) + ' - Test Set',\n",
        "                                                 y_test, y_test_pred, binary, print_enable)\n",
        "\n",
        "    # auc-roc\n",
        "    if binary:\n",
        "        y_test_proba = clf.predict_proba(X_test)[::,1]\n",
        "        y_train_proba = clf.predict_proba(X_train)[::,1]\n",
        "        tr_roc_auc = roc_auc_score(y_train, y_train_proba)\n",
        "        ts_roc_auc = roc_auc_score(y_test, y_test_proba)\n",
        "\n",
        "    split_exp_id = exp_id.split(\";\")\n",
        "    if len(split_exp_id) == 2:\n",
        "        index = split_exp_id[0]\n",
        "        id = split_exp_id[1]\n",
        "    elif len(split_exp_id) == 1:\n",
        "        index = 0\n",
        "        id = split_exp_id[0]\n",
        "    else:\n",
        "        raise ValueError(\"Incorrect Experiment ID!\")\n",
        "\n",
        "    perf_dict = {\n",
        "        'index': index,\n",
        "        'exp_id': id,\n",
        "        'emb_method': str(emb_flag),\n",
        "        'classifier': str(clf_id),\n",
        "\n",
        "        'train_prec': tr_prec,\n",
        "        'train_rec': tr_rec,\n",
        "        'train_f1': tr_f1,\n",
        "        'train_acc': tr_acc,\n",
        "        'train_auc': tr_roc_auc,\n",
        "\n",
        "        'test_prec': ts_prec,\n",
        "        'test_rec': ts_rec,\n",
        "        'test_f1': ts_f1,\n",
        "        'test_acc': ts_acc,\n",
        "        'test_auc': ts_roc_auc\n",
        "    }\n",
        "    print(perf_dict)\n",
        "    return perf_dict, clf\n",
        "\n",
        "\n",
        "def rf_lr_classification(X_train, X_test, y_train, y_test, stats_file, flag,\n",
        "                         binary, exp_id, print_report=False):\n",
        "    \"\"\"\n",
        "    apply classification to input X with label y with \"Random Forest\" & \"Logistic Regression\"\n",
        "    :param X_train: train set\n",
        "    :param X_test: test set\n",
        "    :param y_train: train set labels\n",
        "    :param y_test: test set labels\n",
        "    :param print_report: whether print the results of classification or not\n",
        "    :return the classification results\n",
        "    \"\"\"\n",
        "    # define classifier\n",
        "    rf_clf = RandomForestClassifier(n_estimators=50, max_features=10, max_depth=5, random_state=rnd_seed)\n",
        "    lr_clf = LogisticRegression(penalty='l1', solver='liblinear', max_iter=1e5, random_state=rnd_seed)\n",
        "\n",
        "    # apply classification\n",
        "    rf_perf, rf_clf = simple_classification(rf_clf, 'RF', flag, X_train, X_test, y_train, y_test,\n",
        "                                            binary, exp_id, print_report)\n",
        "    lr_perf, lr_clf = simple_classification(lr_clf, 'LR', flag, X_train, X_test, y_train, y_test,\n",
        "                                            binary, exp_id, print_report)\n",
        "\n",
        "    # append the results to file\n",
        "    # stats_df = pd.read_csv(stats_file)\n",
        "    # stats_df = stats_df.append(rf_perf, ignore_index=True)\n",
        "    # stats_df = stats_df.append(lr_perf, ignore_index=True)\n",
        "    # stats_df.to_csv(stats_file, index=False)\n",
        "\n",
        "    return rf_perf, rf_clf, lr_perf, lr_clf\n",
        "\n",
        "\n",
        "def RF_sorted_feature_importance(clf, feature_name):\n",
        "    \"\"\"\n",
        "    return the top 10 most important features of the RF clf model\n",
        "    assumption: clf is a trained RF model\n",
        "    \"\"\"\n",
        "    # feature importance\n",
        "    importance = clf.feature_importances_\n",
        "    indices = np.argsort(importance)[::-1]\n",
        "\n",
        "    # Print the feature ranking\n",
        "    sorted_feature_name = [feature_name[indices[i]] for i in range(len(feature_name))]\n",
        "    sorted_feature_importance = [importance[indices[i]] for i in range(len(feature_name))]\n",
        "    feature_imp_df = pd.DataFrame(list(zip(sorted_feature_name, sorted_feature_importance)),\n",
        "                                  columns=['feature', 'importance'])\n",
        "    return feature_imp_df\n",
        "\n",
        "\n",
        "def RF_feature_imp(X, y, feature_name, png_file):\n",
        "    \"\"\"\n",
        "    calculate feature importance for the Random Forest Classifier\n",
        "    :param X: features\n",
        "    :param y: labels\n",
        "    :param feature_name: the name of the features\n",
        "    \"\"\"\n",
        "    # define and fit classifier\n",
        "    rf_clf = RandomForestClassifier(n_estimators=100, max_features=16, max_depth=5,\n",
        "                                    random_state=rnd_seed)\n",
        "    rf_clf.fit(X, y)\n",
        "\n",
        "    # feature importance\n",
        "    importances = rf_clf.feature_importances_\n",
        "    std = np.std([tree.feature_importances_ for tree in rf_clf.estimators_], axis=0)\n",
        "    indices = np.argsort(importances)[::-1]\n",
        "\n",
        "    # Print the feature ranking\n",
        "    print(\"Feature ranking:\")\n",
        "    for f in range(len(feature_name)):\n",
        "        print(\"%d. feature %d (%s) (%f)\" % (f + 1, indices[f], feature_name[indices[f]],\n",
        "                                            importances[indices[f]]))\n",
        "\n",
        "    # Plot the impurity-based feature importances of the forest\n",
        "    plt.figure()\n",
        "    plt.title(\"Feature Importance\")\n",
        "    plt.bar(range(len(feature_name)), importances[indices], color=\"g\", yerr=std[indices], align=\"center\")\n",
        "    plt.xticks(range(len(feature_name)), indices)\n",
        "    plt.xlim([-1, len(feature_name)])\n",
        "    # plt.show()\n",
        "    plt.savefig(png_file)\n",
        "\n",
        "\n",
        "def read_emb_and_node_list(emb_file, node_file):\n",
        "    # read embedding\n",
        "    emb_df = pd.read_csv(emb_file, sep=' ', skiprows=1, header=None)\n",
        "    emb_df.columns = ['node'] + [f'emb_{i}' for i in range(emb_df.shape[1] - 1)]\n",
        "\n",
        "    # read node list\n",
        "    node_df = pd.read_csv(node_file)\n",
        "    node_df = node_df[['node', 'isp']]\n",
        "\n",
        "    # merge\n",
        "    merged_df = emb_df.merge(node_df, on='node', how='left')\n",
        "    return merged_df\n",
        "\n",
        "\n",
        "def data_preproc_for_RiWalk_Binary_clf(emb_file, node_file):\n",
        "    \"\"\"\n",
        "    pre-process the RiWalk generated embedding for node classification\n",
        "    \"\"\"\n",
        "    # read and merge the data frames\n",
        "    merged_df = read_emb_and_node_list(emb_file, node_file)\n",
        "\n",
        "    # datasets for  BINARY classification\n",
        "    X = merged_df # only anchor nodes\n",
        "    y = X['isp'].tolist()\n",
        "    X = X.drop(['node', 'isp'], axis=1)\n",
        "    feature_names = X.columns\n",
        "    X = X.values.tolist()\n",
        "\n",
        "    # split the train and test set\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, rnd_seed)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test, feature_names\n",
        "\n",
        "\n",
        "def prepare_data_for_concat_fe_emb(emb_file, fe_file):\n",
        "    \"\"\"\n",
        "    pre-process the data for the node classification of a new dataset consisting of the\n",
        "    engineered features and the embeddings\n",
        "    \"\"\"\n",
        "    # read embedding\n",
        "    emb_df = pd.read_csv(emb_file, sep=' ', skiprows=1, header=None)\n",
        "    emb_df.columns = ['node'] + [f'emb_{i}' for i in range(emb_df.shape[1] - 1)]\n",
        "\n",
        "    # read node list\n",
        "    node_df = pd.read_csv(fe_file)\n",
        "    # scale features\n",
        "    feature_col = [f for f in node_df.columns if f not in ['node', 'isp']]\n",
        "    scaler = StandardScaler()\n",
        "    node_df[feature_col] = scaler.fit_transform(node_df[feature_col])\n",
        "\n",
        "    # merge\n",
        "    merged_df = emb_df.merge(node_df, on='node', how='left')\n",
        "\n",
        "    # datasets for  BINARY classification\n",
        "    X = merged_df  # only anchor nodes\n",
        "    y = X['isp'].tolist()\n",
        "    X = X.drop(['node', 'isp'], axis=1)\n",
        "    X = X.values.tolist()\n",
        "\n",
        "    # split the train and test set\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, rnd_seed)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "\n",
        "def plot_TSNE(values, labels, png_file):\n",
        "    \"\"\"\n",
        "    plot the embeddings as a TSNE graph\n",
        "    \"\"\"\n",
        "    print('\\tt-SNE starts.')\n",
        "    time_start = time.time()\n",
        "    tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
        "    tsne_results = tsne.fit_transform(values)\n",
        "    print('\\tt-SNE done! Time elapsed: {} seconds'.format(time.time() - time_start))\n",
        "\n",
        "    # plotting\n",
        "    p_data = {'tsne-2d-first': tsne_results[:, 0],\n",
        "              'tsne-2d-second': tsne_results[:, 1],\n",
        "              'label': labels,\n",
        "              }\n",
        "\n",
        "    plt.figure(figsize=(16, 10))\n",
        "    sns.scatterplot(\n",
        "        x=\"tsne-2d-first\", y=\"tsne-2d-second\",\n",
        "        hue=\"label\",\n",
        "        palette=sns.color_palette(\"hls\", len(set(labels))),\n",
        "        data=p_data,\n",
        "        legend=\"full\",\n",
        "        alpha=0.3\n",
        "    )\n",
        "    # plt.show()\n",
        "    plt.savefig(png_file)\n",
        "\n",
        "\n",
        "def EF_analysis_selected_nodes(output_path, graph, edges_filename, nodes_filename,\n",
        "                               features_filename, stats_file, feat_imp_filename,\n",
        "                               flag, binary, rnd_seed, exp_id, extra_analysis):\n",
        "    nodes_df = pd.read_csv(nodes_filename)\n",
        "\n",
        "    print(\"\\tRetrieve anchor nodes for classification.\")\n",
        "    start_time = time.time()\n",
        "    selected_node_list = nodes_df['node'].tolist()\n",
        "    print(\"\\t\\tTime elapsed {} seconds.\".format(time.time() - start_time))\n",
        "\n",
        "\n",
        "    print(\"\\tRead features for anchor nodes.\")\n",
        "    start_time = time.time()\n",
        "    all_node_features_df = pd.read_csv(features_filename)\n",
        "    features_df = all_node_features_df.loc[all_node_features_df['node'].isin(selected_node_list)]\n",
        "    print(\"\\t\\tTime elapsed {} seconds.\".format(time.time() - start_time))\n",
        "\n",
        "    # make ready for classification\n",
        "    y = features_df['isp'].tolist()  # only anchor nodes where selected\n",
        "    X_orig = features_df.drop(['node', 'isp'], axis=1)\n",
        "    feature_names = X_orig.columns\n",
        "    X_orig = X_orig.values.tolist()\n",
        "\n",
        "    # split the train and test set\n",
        "    print(\"\\tTrain-Test split.\")\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_orig, y, rnd_seed)\n",
        "\n",
        "    # scale the features; note that it should be fitted on the train set ONLY\n",
        "    print('\\tScaling the features.')\n",
        "    min_max_scaler = MinMaxScaler()\n",
        "    min_max_scaler.fit(X_train)\n",
        "    X_train_scaled = min_max_scaler.transform(X_train)\n",
        "    X_test_scaled = min_max_scaler.transform(X_test)\n",
        "\n",
        "    # classification\n",
        "    print('\\tApplying classification.')\n",
        "    start_time = time.time()\n",
        "    rf_perf, rf_clf, lr_perf, lr_clf = rf_lr_classification(X_train_scaled, X_test_scaled, y_train,\n",
        "                                                            y_test, stats_file, flag, binary,\n",
        "                                                            exp_id, print_report=True)\n",
        "    print(\"\\t\\tTime elapsed {} seconds.\".format(time.time() - start_time))\n",
        "\n",
        "    # calculates and saves features importance\n",
        "    feature_imp_df = RF_sorted_feature_importance(rf_clf, feature_names)\n",
        "    feature_imp_df.to_csv(feat_imp_filename, index=False)\n",
        "\n",
        "    if extra_analysis:\n",
        "        # Feature importance\n",
        "        print(\"\\tInvestigate feature importance.\")\n",
        "        png_file = output_path + '/' + graph + '_' + flag + '_FE_feature_impo.png'\n",
        "        RF_feature_imp(X_train_scaled, y_train, feature_names, png_file)\n",
        "\n",
        "        # plot t-SNE graph\n",
        "        print(\"\\tt-SNE graph.\")\n",
        "        values = X_orig\n",
        "        groups = y\n",
        "        png_file = output_path + '/' + graph + '_' + flag + '_FE_tsne.png'\n",
        "        plot_TSNE(values, groups, png_file)\n",
        "\n",
        "    print(\"FE node classification finished.\")\n",
        "\n",
        "\n",
        "def RiWalk_analysis_selected_nodes(output_path, graph, emb_filename, nodes_filename, stats_filename,\n",
        "                                   flag, binary, exp_id, extra_analysis):\n",
        "    # prepare the data\n",
        "    print(\"\\tPrepare data sets.\")\n",
        "    X_train, X_test, y_train, y_test, feature_names = data_preproc_for_RiWalk_Binary_clf(emb_filename,\n",
        "                                                                                         nodes_filename)\n",
        "    # classification\n",
        "    print('\\tApplying classification.')\n",
        "    start_time = time.time()\n",
        "    rf_lr_classification(X_train, X_test, y_train, y_test, stats_filename, flag,\n",
        "                         binary, exp_id, print_report=True)\n",
        "    print(\"\\tTime elapsed {} seconds.\".format(time.time() - start_time))\n",
        "\n",
        "    if extra_analysis:\n",
        "        # Feature importance\n",
        "        print(\"\\tInvestigate feature importance.\")\n",
        "        png_file = output_path + '/' + graph + '_' + flag + '_Ri_feature_impo.png'\n",
        "        RF_feature_imp(X_train, y_train, feature_names, png_file)\n",
        "\n",
        "        # plot t-SNE graph\n",
        "        print(\"\\tPlot t-SNE.\")\n",
        "        values = X_train + X_test\n",
        "        groups = y_train + y_test\n",
        "        # nodes_df = pd.read_csv(nodes_filename)\n",
        "        png_file = output_path + '/' + graph + flag + '_Ri_tsne.png'\n",
        "        plot_TSNE(values, groups, png_file)\n",
        "\n",
        "    print(\"RiWalk node classification finished.\")\n",
        "\n",
        "\n",
        "def nd_clf_fe_emb_combined(emb_file, fe_file, stats_file, flag, binary, exp_id):\n",
        "    \"\"\"\n",
        "    apply the node classification based on a new feature set constructed by combining the\n",
        "    engineered features and the (structural) embedding generated by an automatic method like node2vec\n",
        "    \"\"\"\n",
        "    print(\"\\tConcatenating embedding with engineered features for node classification.\")\n",
        "    # data preparation\n",
        "    X_train, X_test, y_train, y_test = prepare_data_for_concat_fe_emb(emb_file, fe_file)\n",
        "\n",
        "    # classification\n",
        "    print('\\tApplying classification.')\n",
        "    start_time = time.time()\n",
        "    rf_lr_classification(X_train, X_test, y_train, y_test, stats_file, flag,\n",
        "                         binary, exp_id, print_report=True)\n",
        "    print(\"\\tTime elapsed {} seconds.\".format(time.time() - start_time))\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    end-to-end classification\n",
        "    \"\"\"\n",
        "    binary = True  # binary or multi-class classification.\n",
        "\n",
        "    flag = 'sp'\n",
        "    clf_opt = 'abcd'\n",
        "    exp_id = '1;elliptic'\n",
        "\n",
        "    nodes_filename = \"/content/drive/My Drive/Baseline/Dataset/nodeData.csv\"\n",
        "    edges_filename = \"/content/drive/My Drive/Baseline/Dataset/edgeData.csv\"\n",
        "    features_filename = \"/content/drive/My Drive/Baseline/Dataset/features.csv\"\n",
        "    feat_imp_filename = \"/content/drive/My Drive/Baseline/Dataset/imp_features.csv\"\n",
        "    prod_data_dir = \"/content/drive/My Drive/Baseline/Dataset/Prod\"\n",
        "    graph_filename = 'graph_filename'\n",
        "    stats_file = \"/content/drive/My Drive/Baseline/Dataset/stats.csv\"\n",
        "    emb_filename = \"/content/drive/My Drive/Baseline/Dataset/embeddings.emb\"\n",
        "\n",
        "\n",
        "    if clf_opt == 'fe':\n",
        "        # ------------------ Feature Engineering ------------------\n",
        "        # read the input file and generating the features and the labels set\n",
        "        print(\"Node Classification --- Feature Engineering ---\")\n",
        "\n",
        "        EF_analysis_selected_nodes(prod_data_dir, graph_filename, edges_filename, nodes_filename,\n",
        "                                   features_filename, stats_file, feat_imp_filename, 'FE', binary,\n",
        "                                   rnd_seed, exp_id, extra_analysis=False)\n",
        "        print(\"--- Node Classification Feature Engineering is done ---\")\n",
        "        # ---------------------------------------------------------\n",
        "\n",
        "    elif clf_opt == 'concat':\n",
        "        print(\"Node classification: Concat. FE &\" + flag + \" embeddings.\")\n",
        "\n",
        "        emb_file = \"/content/drive/My Drive/Baseline/Dataset/embeddings.emb\"\n",
        "        fe_file = features_filename\n",
        "        nd_clf_fe_emb_combined(emb_file, fe_file, stats_file, flag, binary, exp_id)\n",
        "\n",
        "    else:\n",
        "        # ------------------ RiWalk -------------------------------\n",
        "        print(\"Node classification: --- RiWalk - \" + flag + \"---\")\n",
        "        RiWalk_analysis_selected_nodes(prod_data_dir, graph_filename, emb_filename, nodes_filename, stats_file,\n",
        "                                       flag, binary, exp_id, extra_analysis=False)\n",
        "        print(\"--- Classification RiWalk is done ---\")\n",
        "        # ---------------------------------------------------------\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "zh-FEA6I943S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4ce8571-6557-4187-b060-4aa9d3603da2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Node classification: --- RiWalk - sp---\n",
            "\tPrepare data sets.\n",
            "\tApplying classification.\n",
            "C\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "D\n",
            "\t*** RF - Training Set performance reports: ***\n",
            "\t\tPrecision: 0.000 \n",
            "\t\tRecall: 0.000 \n",
            "\t\tF1-Score: 0.000\n",
            "\t\tMicro-Average F1-Score: 0.000\n",
            "\t\tAccuracy: 0.967\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      1.00      0.98     27401\n",
            "           1       0.00      0.00      0.00       932\n",
            "\n",
            "    accuracy                           0.97     28333\n",
            "   macro avg       0.48      0.50      0.49     28333\n",
            "weighted avg       0.94      0.97      0.95     28333\n",
            "\n",
            "\t*** RF - Test Set performance reports: ***\n",
            "\t\tPrecision: 0.000 \n",
            "\t\tRecall: 0.000 \n",
            "\t\tF1-Score: 0.000\n",
            "\t\tMicro-Average F1-Score: 0.000\n",
            "\t\tAccuracy: 0.967\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      1.00      0.98      6851\n",
            "           1       0.00      0.00      0.00       233\n",
            "\n",
            "    accuracy                           0.97      7084\n",
            "   macro avg       0.48      0.50      0.49      7084\n",
            "weighted avg       0.94      0.97      0.95      7084\n",
            "\n",
            "{'index': '1', 'exp_id': 'elliptic', 'emb_method': 'sp', 'classifier': 'RF', 'train_prec': 0.0, 'train_rec': 0.0, 'train_f1': 0.0, 'train_acc': 0.9671054953587689, 'train_auc': 0.6824987434279598, 'test_prec': 0.0, 'test_rec': 0.0, 'test_f1': 0.0, 'test_acc': 0.9671089779785432, 'test_auc': 0.4915121566789849}\n",
            "C\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t*** LR - Training Set performance reports: ***\n",
            "\t\tPrecision: 0.000 \n",
            "\t\tRecall: 0.000 \n",
            "\t\tF1-Score: 0.000\n",
            "\t\tMicro-Average F1-Score: 0.000\n",
            "\t\tAccuracy: 0.967\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      1.00      0.98     27401\n",
            "           1       0.00      0.00      0.00       932\n",
            "\n",
            "    accuracy                           0.97     28333\n",
            "   macro avg       0.48      0.50      0.49     28333\n",
            "weighted avg       0.94      0.97      0.95     28333\n",
            "\n",
            "\t*** LR - Test Set performance reports: ***\n",
            "\t\tPrecision: 0.000 \n",
            "\t\tRecall: 0.000 \n",
            "\t\tF1-Score: 0.000\n",
            "\t\tMicro-Average F1-Score: 0.000\n",
            "\t\tAccuracy: 0.967\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      1.00      0.98      6851\n",
            "           1       0.00      0.00      0.00       233\n",
            "\n",
            "    accuracy                           0.97      7084\n",
            "   macro avg       0.48      0.50      0.49      7084\n",
            "weighted avg       0.94      0.97      0.95      7084\n",
            "\n",
            "{'index': '1', 'exp_id': 'elliptic', 'emb_method': 'sp', 'classifier': 'LR', 'train_prec': 0.0, 'train_rec': 0.0, 'train_f1': 0.0, 'train_acc': 0.9671054953587689, 'train_auc': 0.5, 'test_prec': 0.0, 'test_rec': 0.0, 'test_f1': 0.0, 'test_acc': 0.9671089779785432, 'test_auc': 0.5}\n",
            "\tTime elapsed 13.877276420593262 seconds.\n",
            "RiWalk node classification finished.\n",
            "--- Classification RiWalk is done ---\n"
          ]
        }
      ]
    }
  ]
}