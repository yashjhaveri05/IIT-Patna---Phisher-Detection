{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SIGTRAN-Features.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPx/c5VNoQKJWgi+FOz0GcL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"08P0bJ7hC56T","executionInfo":{"status":"ok","timestamp":1639047366293,"user_tz":-330,"elapsed":3759,"user":{"displayName":"IIT_Patna Internship","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04579180012960262730"}},"outputId":"1e7ebf06-6a81-4836-c22c-16172a7e8a57"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","metadata":{"id":"_oQ-iCPSDEyR"},"source":["import pandas as pd\n","import numpy as np\n","from matplotlib import pyplot as plt\n","import random\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import classification_report\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.preprocessing import MinMaxScaler, StandardScaler\n","from sklearn.model_selection import StratifiedShuffleSplit\n","from sklearn.metrics import f1_score, precision_recall_fscore_support, accuracy_score, roc_auc_score\n","from sklearn.manifold import TSNE\n","import time\n","import seaborn as sns\n","\n","rnd_seed = 42\n","random.seed(rnd_seed)\n","test_size = 0.2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AlOLAd76Da-6"},"source":["def perf_report(identifier, y_true, y_pred, binary, print_enable=False):\n","    if binary:\n","        # print(\">>> Binary Classification.\")\n","        prec, rec, f1, num = precision_recall_fscore_support(y_true, y_pred, average='binary')\n","        micro_f1 = f1_score(y_true, y_pred, average='binary')\n","    else:\n","        print(\">>> Multi-class Classification.\")\n","        prec, rec, f1, num = precision_recall_fscore_support(y_true, y_pred, average='macro')\n","        micro_f1 = f1_score(y_true, y_pred, average='macro')\n","    acc = accuracy_score(y_true, y_pred)\n","    if print_enable:\n","        print(\"\\t*** {} performance reports: ***\".format(str(identifier)))\n","        print(\"\\t\\tPrecision: %.3f \\n\\t\\tRecall: %.3f \\n\\t\\tF1-Score: %.3f\" % (prec, rec, f1))\n","        print('\\t\\tMicro-Average F1-Score: %.3f' % micro_f1)\n","        print('\\t\\tAccuracy: %.3f' % acc)\n","        print(classification_report(y_true, y_pred))\n","    return prec, rec, f1, acc"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LI-pd8HVDfyG"},"source":["def train_test_split(X, y, rnd_seed):\n","    \"\"\"\n","    split the features and the labels according to the indices\n","    :param X: feature set, should be array or list\n","    :param y: labels, should be array or list\n","    :param rnd_seed: random seed\n","    \"\"\"\n","    # generate indices for the train and test set\n","    indices = [i for i in range(len(y))]\n","    sss = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=rnd_seed)\n","    sss.get_n_splits(indices, y)\n","    train_indices, test_indices = next(sss.split(indices, y))\n","\n","    # train/test split\n","    X_train = [X[i] for i in train_indices]\n","    X_test = [X[i] for i in test_indices]\n","\n","    y_train = [y[i] for i in train_indices]\n","    y_test = [y[i] for i in test_indices]\n","\n","    return X_train, X_test, y_train, y_test"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3JLaR4pYDf5H"},"source":["def simple_classification(clf, clf_id, emb_flag, X_train, X_test, y_train, y_test,\n","                          binary, exp_id, print_enable=False):\n","    \"\"\"\n","    train the model on the train set and test it on the test set.\n","    to be consistent among different run, the indices are passed.\n","    important NOTE: it is implicitly inferred that the positive label is 1.\n","    no cross-validation is applied.\n","    \"\"\"\n","\n","    # train the model\n","    clf.fit(X_train, y_train)\n","\n","    # predict the training set labels\n","    y_train_pred = clf.predict(X_train)\n","\n","    # predict the test set labels\n","    y_test_pred = clf.predict(X_test)\n","\n","    # evaluate the performance for the training set\n","    tr_prec, tr_rec, tr_f1, tr_acc = perf_report(str(clf_id) + ' - Training Set',\n","                                                 y_train, y_train_pred, binary, print_enable)\n","    ts_prec, ts_rec, ts_f1, ts_acc = perf_report(str(clf_id) + ' - Test Set',\n","                                                 y_test, y_test_pred, binary, print_enable)\n","\n","    # auc-roc\n","    if binary:\n","        tr_roc_auc = roc_auc_score(y_train, clf.predict_proba(X_train)[:, 1])\n","        ts_roc_auc = roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1])\n","    else:\n","        tr_roc_auc = roc_auc_score(y_train, clf.predict_proba(X_train), multi_class='ovr')\n","        ts_roc_auc = roc_auc_score(y_test, clf.predict_proba(X_test), multi_class='ovr')\n","\n","    split_exp_id = exp_id.split(\";\")\n","    if len(split_exp_id) == 2:\n","        index = split_exp_id[0]\n","        id = split_exp_id[1]\n","    elif len(split_exp_id) == 1:\n","        index = 0\n","        id = split_exp_id[0]\n","    else:\n","        raise ValueError(\"Incorrect Experiment ID!\")\n","\n","    perf_dict = {\n","        'index': index,\n","        'exp_id': id,\n","        'emb_method': str(emb_flag),\n","        'classifier': str(clf_id),\n","\n","        'train_prec': tr_prec,\n","        'train_rec': tr_rec,\n","        'train_f1': tr_f1,\n","        'train_acc': tr_acc,\n","        'train_auc': tr_roc_auc,\n","\n","        'test_prec': ts_prec,\n","        'test_rec': ts_rec,\n","        'test_f1': ts_f1,\n","        'test_acc': ts_acc,\n","        'test_auc': ts_roc_auc\n","    }\n","\n","    return perf_dict, clf"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oBQI9W_XDmOA"},"source":["def rf_lr_classification(X_train, X_test, y_train, y_test, stats_file, flag,\n","                         binary, exp_id, print_report=False):\n","    \"\"\"\n","    apply classification to input X with label y with \"Random Forest\" & \"Logistic Regression\"\n","    :param X_train: train set\n","    :param X_test: test set\n","    :param y_train: train set labels\n","    :param y_test: test set labels\n","    :param print_report: whether print the results of classification or not\n","    :return the classification results\n","    \"\"\"\n","    # define classifier\n","    rf_clf = RandomForestClassifier(n_estimators=50, max_features=10, max_depth=5, random_state=rnd_seed)\n","    lr_clf = LogisticRegression(penalty='l1', solver='liblinear', max_iter=1e5, random_state=rnd_seed)\n","\n","    # apply classification\n","    rf_perf, rf_clf = simple_classification(rf_clf, 'RF', flag, X_train, X_test, y_train, y_test,\n","                                            binary, exp_id, print_report)\n","    lr_perf, lr_clf = simple_classification(lr_clf, 'LR', flag, X_train, X_test, y_train, y_test,\n","                                            binary, exp_id, print_report)\n","\n","    # append the results to file\n","    stats_df = pd.read_csv(stats_file)\n","    stats_df = stats_df.append(rf_perf, ignore_index=True)\n","    stats_df = stats_df.append(lr_perf, ignore_index=True)\n","    stats_df.to_csv(stats_file, index=False)\n","\n","    return rf_perf, rf_clf, lr_perf, lr_clf"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"58BmOhjuDz3A"},"source":["def RF_sorted_feature_importance(clf, feature_name):\n","    \"\"\"\n","    return the top 10 most important features of the RF clf model\n","    assumption: clf is a trained RF model\n","    \"\"\"\n","    # feature importance\n","    importance = clf.feature_importances_\n","    indices = np.argsort(importance)[::-1]\n","\n","    # Print the feature ranking\n","    sorted_feature_name = [feature_name[indices[i]] for i in range(len(feature_name))]\n","    sorted_feature_importance = [importance[indices[i]] for i in range(len(feature_name))]\n","    feature_imp_df = pd.DataFrame(list(zip(sorted_feature_name, sorted_feature_importance)),\n","                                  columns=['feature', 'importance'])\n","    return feature_imp_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0DVNFVSeD2-m"},"source":["def RF_feature_imp(X, y, feature_name, png_file):\n","    \"\"\"\n","    calculate feature importance for the Random Forest Classifier\n","    :param X: features\n","    :param y: labels\n","    :param feature_name: the name of the features\n","    \"\"\"\n","    # define and fit classifier\n","    rf_clf = RandomForestClassifier(n_estimators=100, max_features=16, max_depth=5,\n","                                    random_state=rnd_seed)\n","    rf_clf.fit(X, y)\n","\n","    # feature importance\n","    importances = rf_clf.feature_importances_\n","    std = np.std([tree.feature_importances_ for tree in rf_clf.estimators_], axis=0)\n","    indices = np.argsort(importances)[::-1]\n","\n","    # Print the feature ranking\n","    print(\"Feature ranking:\")\n","    for f in range(len(feature_name)):\n","        print(\"%d. feature %d (%s) (%f)\" % (f + 1, indices[f], feature_name[indices[f]],\n","                                            importances[indices[f]]))\n","\n","    # Plot the impurity-based feature importances of the forest\n","    plt.figure()\n","    plt.title(\"Feature Importance\")\n","    plt.bar(range(len(feature_name)), importances[indices], color=\"g\", yerr=std[indices], align=\"center\")\n","    plt.xticks(range(len(feature_name)), indices)\n","    plt.xlim([-1, len(feature_name)])\n","    # plt.show()\n","    plt.savefig(png_file)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PEarH8v0D3k1"},"source":["def read_emb_and_node_list(emb_file, node_file):\n","    # read embedding\n","    emb_df = pd.read_csv(emb_file, sep=' ', skiprows=1, header=None)\n","    emb_df.columns = ['node'] + [f'emb_{i}' for i in range(emb_df.shape[1] - 1)]\n","\n","    # read node list\n","    node_df = pd.read_csv(node_file)\n","    node_df = node_df[['node', 'isp']]\n","\n","    # merge\n","    merged_df = emb_df.merge(node_df, on='node', how='left')\n","\n","    return merged_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sJ1I-sNqD7cW"},"source":["def data_preproc_for_RiWalk_Binary_clf(emb_file, node_file):\n","    \"\"\"\n","    pre-process the RiWalk generated embedding for node classification\n","    \"\"\"\n","    # read and merge the data frames\n","    merged_df = read_emb_and_node_list(emb_file, node_file)\n","\n","    # datasets for  BINARY classification\n","    X = merged_df # only anchor nodes\n","    y = X['isp'].tolist()\n","    X = X.drop(['node', 'isp'], axis=1)\n","    feature_names = X.columns\n","    X = X.values.tolist()\n","\n","    # split the train and test set\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, rnd_seed)\n","\n","    return X_train, X_test, y_train, y_test, feature_names"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uwpiriAHEB_f"},"source":["def prepare_data_for_concat_fe_emb(emb_file, fe_file):\n","    \"\"\"\n","    pre-process the data for the node classification of a new dataset consisting of the\n","    engineered features and the embeddings\n","    \"\"\"\n","    # read embedding\n","    emb_df = pd.read_csv(emb_file, sep=' ', skiprows=1, header=None)\n","    emb_df.columns = ['node'] + [f'emb_{i}' for i in range(emb_df.shape[1] - 1)]\n","\n","    # read node list\n","    node_df = pd.read_csv(fe_file)\n","    # scale features\n","    feature_col = [f for f in node_df.columns if f not in ['node', 'isp']]\n","    scaler = StandardScaler()\n","    node_df[feature_col] = scaler.fit_transform(node_df[feature_col])\n","\n","    # merge\n","    merged_df = emb_df.merge(node_df, on='node', how='left')\n","\n","    # datasets for  BINARY classification\n","    X = merged_df  # only anchor nodes\n","    y = X['isp'].tolist()\n","    X = X.drop(['node', 'isp'], axis=1)\n","    X = X.values.tolist()\n","\n","    # split the train and test set\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, rnd_seed)\n","\n","    return X_train, X_test, y_train, y_test"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OGsQ_hUmECB2"},"source":["def plot_TSNE(values, labels, png_file):\n","    \"\"\"\n","    plot the embeddings as a TSNE graph\n","    \"\"\"\n","    print('\\tt-SNE starts.')\n","    time_start = time.time()\n","    tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n","    tsne_results = tsne.fit_transform(values)\n","    print('\\tt-SNE done! Time elapsed: {} seconds'.format(time.time() - time_start))\n","\n","    # plotting\n","    p_data = {'tsne-2d-first': tsne_results[:, 0],\n","              'tsne-2d-second': tsne_results[:, 1],\n","              'label': labels,\n","              }\n","\n","    plt.figure(figsize=(16, 10))\n","    sns.scatterplot(\n","        x=\"tsne-2d-first\", y=\"tsne-2d-second\",\n","        hue=\"label\",\n","        palette=sns.color_palette(\"hls\", len(set(labels))),\n","        data=p_data,\n","        legend=\"full\",\n","        alpha=0.3\n","    )\n","    # plt.show()\n","    plt.savefig(png_file)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6hjjnAPQECEa"},"source":["def EF_analysis_selected_nodes(output_path, graph, edges_filename, nodes_filename,\n","                               features_filename, stats_file, feat_imp_filename,\n","                               flag, binary, rnd_seed, exp_id, extra_analysis):\n","    # print(\"\\tRead edge list and node list.\")\n","    # start_time = time.time()\n","    # edges_df = pd.read_csv(edges_filename)\n","    nodes_df = pd.read_csv(nodes_filename)\n","    # print(\"\\t\\tTime elapsed {} seconds.\".format(time.time() - start_time))\n","\n","    print(\"\\tRetrieve anchor nodes for classification.\")\n","    start_time = time.time()\n","    selected_node_list = nodes_df['node'].tolist()\n","    print(\"\\t\\tTime elapsed {} seconds.\".format(time.time() - start_time))\n","\n","\n","    print(\"\\tRead features for anchor nodes.\")\n","    start_time = time.time()\n","    all_node_features_df = pd.read_csv(features_filename)\n","    features_df = all_node_features_df.loc[all_node_features_df['node'].isin(selected_node_list)]\n","    print(\"\\t\\tTime elapsed {} seconds.\".format(time.time() - start_time))\n","\n","    # make ready for classification\n","    # features_df = pd.read_csv(features_filename)\n","    y = features_df['isp'].tolist()  # only anchor nodes where selected\n","    X_orig = features_df.drop(['node', 'isp'], axis=1)\n","    feature_names = X_orig.columns\n","    X_orig = X_orig.values.tolist()\n","\n","    # split the train and test set\n","    print(\"\\tTrain-Test split.\")\n","    X_train, X_test, y_train, y_test = train_test_split(X_orig, y, rnd_seed)\n","\n","    # scale the features; note that it should be fitted on the train set ONLY\n","    print('\\tScaling the features.')\n","    min_max_scaler = MinMaxScaler()\n","    min_max_scaler.fit(X_train)\n","    X_train_scaled = min_max_scaler.transform(X_train)\n","    X_test_scaled = min_max_scaler.transform(X_test)\n","\n","    # classification\n","    print('\\tApplying classification.')\n","    start_time = time.time()\n","    rf_perf, rf_clf, lr_perf, lr_clf = rf_lr_classification(X_train_scaled, X_test_scaled, y_train,\n","                                                            y_test, stats_file, flag, binary,\n","                                                            exp_id, print_report=True)\n","    print(\"\\t\\tTime elapsed {} seconds.\".format(time.time() - start_time))\n","\n","    # calculates and saves features importance\n","    feature_imp_df = RF_sorted_feature_importance(rf_clf, feature_names)\n","    feature_imp_df.to_csv(feat_imp_filename, index=False)\n","\n","    if extra_analysis:\n","        # Feature importance\n","        print(\"\\tInvestigate feature importance.\")\n","        png_file = output_path + '/' + graph + '_' + flag + '_FE_feature_impo.png'\n","        RF_feature_imp(X_train_scaled, y_train, feature_names, png_file)\n","\n","        # plot t-SNE graph\n","        print(\"\\tt-SNE graph.\")\n","        values = X_orig\n","        groups = y\n","        png_file = output_path + '/' + graph + '_' + flag + '_FE_tsne.png'\n","        plot_TSNE(values, groups, png_file)\n","\n","    print(\"FE node classification finished.\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g71bgI2RECGZ"},"source":["def RiWalk_analysis_selected_nodes(output_path, graph, emb_filename, nodes_filename, stats_filename,\n","                                   flag, binary, exp_id, extra_analysis):\n","    # prepare the data\n","    print(\"\\tPrepare data sets.\")\n","    X_train, X_test, y_train, y_test, feature_names = data_preproc_for_RiWalk_Binary_clf(emb_filename,\n","                                                                                         nodes_filename)\n","    # classification\n","    print('\\tApplying classification.')\n","    start_time = time.time()\n","    rf_lr_classification(X_train, X_test, y_train, y_test, stats_filename, flag,\n","                         binary, exp_id, print_report=True)\n","    print(\"\\tTime elapsed {} seconds.\".format(time.time() - start_time))\n","\n","    if extra_analysis:\n","        # Feature importance\n","        print(\"\\tInvestigate feature importance.\")\n","        png_file = output_path + '/' + graph + '_' + flag + '_Ri_feature_impo.png'\n","        RF_feature_imp(X_train, y_train, feature_names, png_file)\n","\n","        # plot t-SNE graph\n","        print(\"\\tPlot t-SNE.\")\n","        values = X_train + X_test\n","        groups = y_train + y_test\n","        # nodes_df = pd.read_csv(nodes_filename)\n","        png_file = output_path + '/' + graph + flag + '_Ri_tsne.png'\n","        plot_TSNE(values, groups, png_file)\n","\n","    print(\"RiWalk node classification finished.\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X6eKe6V3ECIk"},"source":["def nd_clf_fe_emb_combined(emb_file, fe_file, stats_file, flag, binary, exp_id):\n","    \"\"\"\n","    apply the node classification based on a new feature set constructed by combining the\n","    engineered features and the (structural) embedding generated by an automatic method like node2vec\n","    \"\"\"\n","    print(\"\\tConcatenating embedding with engineered features for node classification.\")\n","    # data preparation\n","    X_train, X_test, y_train, y_test = prepare_data_for_concat_fe_emb(emb_file, fe_file)\n","\n","    # classification\n","    print('\\tApplying classification.')\n","    start_time = time.time()\n","    rf_lr_classification(X_train, X_test, y_train, y_test, stats_file, flag,\n","                         binary, exp_id, print_report=True)\n","    print(\"\\tTime elapsed {} seconds.\".format(time.time() - start_time))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FU2svQ8GECKj","executionInfo":{"status":"ok","timestamp":1639047634003,"user_tz":-330,"elapsed":1085,"user":{"displayName":"IIT_Patna Internship","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04579180012960262730"}},"outputId":"2da7e0ef-8584-4377-d379-46e3ebc4004d"},"source":["def main():\n","    binary = True\n","    flag = 'sp'\n","    clf_opt = 'fe'\n","    exp_id = '1;elliptic'\n","\n","    nodes_filename = \"/content/drive/My Drive/Initial CSV for FeatureEngineering/nodeUndersampling_final.csv\"\n","    edges_filename = \"/content/drive/My Drive/Initial CSV for FeatureEngineering/edgeUndersampling_final.csv\"\n","    features_filename = \"/content/drive/My Drive/FeatureEngineering/features_undersampling_final.csv\"\n","    feat_imp_filename = \"/content/drive/My Drive/FeatureEngineering/imp_features_undersampling_final.csv\"\n","    prod_data_dir = \"/content/drive/My Drive/SIGTRAN/\"\n","    graph_filename = 'graph_filename'\n","    stats_file = \"/content/drive/My Drive/SIGTRAN/stats_SIGTRAN_undersampling.csv\"\n","    feat_imp_filename = \"/content/drive/My Drive/FeatureEngineering/imp_features_undersampling_final.csv\"\n","\n","    if clf_opt == 'fe':\n","        # ------------------ Feature Engineering ------------------\n","        # read the input file and generating the features and the labels set\n","        print(\"Node Classification --- Feature Engineering ---\")\n","\n","        EF_analysis_selected_nodes(prod_data_dir, graph_filename, edges_filename, nodes_filename,\n","                                   features_filename, stats_file, feat_imp_filename, 'FE', binary,\n","                                   rnd_seed, exp_id, extra_analysis=False)\n","        print(\"--- Node Classification Feature Engineering is done ---\")\n","        # ---------------------------------------------------------\n","\n","    elif clf_opt == 'concat':\n","        print(\"Node classification: Concat. FE &\" + flag + \" embeddings.\")\n","\n","        emb_file = prod_data_dir + 'emb_' + str(flag) + '_' + graph_filename + '.emb'\n","        fe_file = features_filename\n","        nd_clf_fe_emb_combined(emb_file, fe_file, stats_file, flag, binary, exp_id)\n","\n","    else:\n","        # ------------------ RiWalk -------------------------------\n","        print(\"Node classification: --- RiWalk - \" + flag + \"---\")\n","\n","        # set file names\n","        emb_filename = prod_data_dir + 'emb_' + str(flag) + '_' + graph_filename + '.emb'\n","\n","        RiWalk_analysis_selected_nodes(prod_data_dir, graph_filename, emb_filename, nodes_filename, stats_file,\n","                                       flag, binary, exp_id, extra_analysis=False)\n","        print(\"--- Classification RiWalk is done ---\")\n","        # ---------------------------------------------------------\n","\n","\n","if __name__ == '__main__':\n","    main()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Node Classification --- Feature Engineering ---\n","\tRetrieve anchor nodes for classification.\n","\t\tTime elapsed 0.00040221214294433594 seconds.\n","\tRead features for anchor nodes.\n","\t\tTime elapsed 0.22199702262878418 seconds.\n","\tTrain-Test split.\n","\tScaling the features.\n","\tApplying classification.\n","\t*** RF - Training Set performance reports: ***\n","\t\tPrecision: 0.914 \n","\t\tRecall: 0.931 \n","\t\tF1-Score: 0.922\n","\t\tMicro-Average F1-Score: 0.922\n","\t\tAccuracy: 0.922\n","              precision    recall  f1-score   support\n","\n","           0       0.93      0.91      0.92       932\n","           1       0.91      0.93      0.92       932\n","\n","    accuracy                           0.92      1864\n","   macro avg       0.92      0.92      0.92      1864\n","weighted avg       0.92      0.92      0.92      1864\n","\n","\t*** RF - Test Set performance reports: ***\n","\t\tPrecision: 0.933 \n","\t\tRecall: 0.901 \n","\t\tF1-Score: 0.917\n","\t\tMicro-Average F1-Score: 0.917\n","\t\tAccuracy: 0.918\n","              precision    recall  f1-score   support\n","\n","           0       0.90      0.94      0.92       233\n","           1       0.93      0.90      0.92       233\n","\n","    accuracy                           0.92       466\n","   macro avg       0.92      0.92      0.92       466\n","weighted avg       0.92      0.92      0.92       466\n","\n","\t*** LR - Training Set performance reports: ***\n","\t\tPrecision: 0.845 \n","\t\tRecall: 0.621 \n","\t\tF1-Score: 0.716\n","\t\tMicro-Average F1-Score: 0.716\n","\t\tAccuracy: 0.754\n","              precision    recall  f1-score   support\n","\n","           0       0.70      0.89      0.78       932\n","           1       0.85      0.62      0.72       932\n","\n","    accuracy                           0.75      1864\n","   macro avg       0.77      0.75      0.75      1864\n","weighted avg       0.77      0.75      0.75      1864\n","\n","\t*** LR - Test Set performance reports: ***\n","\t\tPrecision: 0.851 \n","\t\tRecall: 0.635 \n","\t\tF1-Score: 0.727\n","\t\tMicro-Average F1-Score: 0.727\n","\t\tAccuracy: 0.762\n","              precision    recall  f1-score   support\n","\n","           0       0.71      0.89      0.79       233\n","           1       0.85      0.64      0.73       233\n","\n","    accuracy                           0.76       466\n","   macro avg       0.78      0.76      0.76       466\n","weighted avg       0.78      0.76      0.76       466\n","\n","\t\tTime elapsed 0.42609143257141113 seconds.\n","FE node classification finished.\n","--- Node Classification Feature Engineering is done ---\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"Cy6yxtW3Bi-s"},"execution_count":null,"outputs":[]}]}