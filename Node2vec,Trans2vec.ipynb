{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Node2vec,Trans2vec.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTge6KIFYslb",
        "outputId": "6644ae83-c169-4586-c3ef-9ca03efa0891"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oQ-iCPSDEyR"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import random\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import OneClassSVM, SVC\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.metrics import f1_score, precision_recall_fscore_support, accuracy_score, roc_auc_score\n",
        "from sklearn.manifold import TSNE\n",
        "import time\n",
        "import seaborn as sns\n",
        "\n",
        "rnd_seed = 42\n",
        "random.seed(rnd_seed)\n",
        "test_size = 0.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AlOLAd76Da-6"
      },
      "source": [
        "def perf_report(identifier, y_true, y_pred, binary, print_enable=False):\n",
        "    if binary:\n",
        "        print(\">>> Binary Classification.\")\n",
        "        prec, rec, f1, num = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
        "        micro_f1 = f1_score(y_true, y_pred, average='binary')\n",
        "    else:\n",
        "        print(\">>> Multi-class Classification.\")\n",
        "        prec, rec, f1, num = precision_recall_fscore_support(y_true, y_pred, average='macro')\n",
        "        micro_f1 = f1_score(y_true, y_pred, average='macro')\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    if print_enable:\n",
        "        print(\"\\t*** {} performance reports: ***\".format(str(identifier)))\n",
        "        print(\"\\t\\tPrecision: %.3f \\n\\t\\tRecall: %.3f \\n\\t\\tF1-Score: %.3f\" % (prec, rec, f1))\n",
        "        print('\\t\\tMicro-Average F1-Score: %.3f' % micro_f1)\n",
        "        print('\\t\\tAccuracy: %.3f' % acc)\n",
        "        print(classification_report(y_true, y_pred))\n",
        "    return prec, rec, f1, acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LI-pd8HVDfyG"
      },
      "source": [
        "def train_test_split(X, y, rnd_seed):\n",
        "    \"\"\"\n",
        "    split the features and the labels according to the indices\n",
        "    :param X: feature set, should be array or list\n",
        "    :param y: labels, should be array or list\n",
        "    :param rnd_seed: random seed\n",
        "    \"\"\"\n",
        "    # generate indices for the train and test set\n",
        "    indices = [i for i in range(len(y))]\n",
        "    sss = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=rnd_seed)\n",
        "    sss.get_n_splits(indices, y)\n",
        "    train_indices, test_indices = next(sss.split(indices, y))\n",
        "\n",
        "    # train/test split\n",
        "    X_train = [X[i] for i in train_indices]\n",
        "    X_test = [X[i] for i in test_indices]\n",
        "\n",
        "    y_train = [y[i] for i in train_indices]\n",
        "    y_test = [y[i] for i in test_indices]\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "def simple_classification(clf, clf_id, emb_flag, X_train, X_test, y_train, y_test,\n",
        "                          binary, exp_id, print_enable=False):\n",
        "    \"\"\"\n",
        "    train the model on the train set and test it on the test set.\n",
        "    to be consistent among different run, the indices are passed.\n",
        "    important NOTE: it is implicitly inferred that the positive label is 1.\n",
        "    no cross-validation is applied.\n",
        "    \"\"\"\n",
        "\n",
        "    # train the model\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    # predict the training set labels\n",
        "    y_train_pred = clf.predict(X_train)\n",
        "\n",
        "    # predict the test set labels\n",
        "    y_test_pred = clf.predict(X_test)\n",
        "\n",
        "    # evaluate the performance for the training set\n",
        "    tr_prec, tr_rec, tr_f1, tr_acc = perf_report(str(clf_id) + ' - Training Set',\n",
        "                                                 y_train, y_train_pred, binary, print_enable)\n",
        "    ts_prec, ts_rec, ts_f1, ts_acc = perf_report(str(clf_id) + ' - Test Set',\n",
        "                                                 y_test, y_test_pred, binary, print_enable)\n",
        "\n",
        "    # # auc-roc\n",
        "    if binary:\n",
        "        y_test_proba = clf.predict_proba(X_test)[::,1]\n",
        "        y_train_proba = clf.predict_proba(X_train)[::,1]\n",
        "        tr_roc_auc = roc_auc_score(y_train, y_train_proba)\n",
        "        ts_roc_auc = roc_auc_score(y_test, y_test_proba)\n",
        "    # else:\n",
        "    #     tr_roc_auc = roc_auc_score(y_train, clf.predict_proba(X_train), multi_class='ovr')\n",
        "    #     ts_roc_auc = roc_auc_score(y_test, clf.predict_proba(X_test), multi_class='ovr')\n",
        "\n",
        "    split_exp_id = exp_id.split(\";\")\n",
        "    if len(split_exp_id) == 2:\n",
        "        index = split_exp_id[0]\n",
        "        id = split_exp_id[1]\n",
        "    elif len(split_exp_id) == 1:\n",
        "        index = 0\n",
        "        id = split_exp_id[0]\n",
        "    else:\n",
        "        raise ValueError(\"Incorrect Experiment ID!\")\n",
        "\n",
        "    perf_dict = {\n",
        "        'index': index,\n",
        "        'exp_id': id,\n",
        "        'emb_method': str(emb_flag),\n",
        "        'classifier': str(clf_id),\n",
        "\n",
        "        'train_prec': tr_prec,\n",
        "        'train_rec': tr_rec,\n",
        "        'train_f1': tr_f1,\n",
        "        'train_acc': tr_acc,\n",
        "        'train_auc': tr_roc_auc,\n",
        "\n",
        "        'test_prec': ts_prec,\n",
        "        'test_rec': ts_rec,\n",
        "        'test_f1': ts_f1,\n",
        "        'test_acc': ts_acc,\n",
        "        'test_auc': ts_roc_auc\n",
        "    }\n",
        "\n",
        "    print(perf_dict)\n",
        "\n",
        "    return perf_dict, clf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBPnXX1CrWqQ"
      },
      "source": [
        "def rf_lr_classification(X_train, X_test, y_train, y_test, stats_file, flag,\n",
        "                         binary, exp_id, print_report=False):\n",
        "    \"\"\"\n",
        "    apply classification to input X with label y with \"Random Forest\" & \"Logistic Regression\"\n",
        "    :param X_train: train set\n",
        "    :param X_test: test set\n",
        "    :param y_train: train set labels\n",
        "    :param y_test: test set labels\n",
        "    :param print_report: whether print the results of classification or not\n",
        "    :return the classification results\n",
        "    \"\"\"\n",
        "    # define classifier\n",
        "    rf_clf = RandomForestClassifier(n_estimators=50, max_features=10, max_depth=5, random_state=rnd_seed)\n",
        "    lr_clf = LogisticRegression(penalty='l1', solver='liblinear', max_iter=1e5, random_state=rnd_seed)\n",
        "\n",
        "    # apply classification\n",
        "    rf_perf, rf_clf = simple_classification(rf_clf, 'RF', flag, X_train, X_test, y_train, y_test,\n",
        "                                            binary, exp_id, print_report)\n",
        "    binary = True\n",
        "    lr_perf, lr_clf = simple_classification(lr_clf, 'LR', flag, X_train, X_test, y_train, y_test,\n",
        "                                            binary, exp_id, print_report)\n",
        "\n",
        "    # append the results to file\n",
        "    # stats_df = pd.read_csv(stats_file)\n",
        "    # stats_df = stats_df.append(rf_perf, ignore_index=True)\n",
        "    # stats_df = stats_df.append(lr_perf, ignore_index=True)\n",
        "    # stats_df.to_csv(stats_file, index=False)\n",
        "\n",
        "    return rf_perf, rf_clf, lr_perf, lr_clf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7W7KOQRrapI"
      },
      "source": [
        "def RF_sorted_feature_importance(clf, feature_name):\n",
        "    \"\"\"\n",
        "    return the top 10 most important features of the RF clf model\n",
        "    assumption: clf is a trained RF model\n",
        "    \"\"\"\n",
        "    # feature importance\n",
        "    importance = clf.feature_importances_\n",
        "    indices = np.argsort(importance)[::-1]\n",
        "\n",
        "    # Print the feature ranking\n",
        "    sorted_feature_name = [feature_name[indices[i]] for i in range(len(feature_name))]\n",
        "    sorted_feature_importance = [importance[indices[i]] for i in range(len(feature_name))]\n",
        "    feature_imp_df = pd.DataFrame(list(zip(sorted_feature_name, sorted_feature_importance)),\n",
        "                                  columns=['feature', 'importance'])\n",
        "    return feature_imp_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29a9qtHgrclQ"
      },
      "source": [
        "def RF_feature_imp(X, y, feature_name, png_file):\n",
        "    \"\"\"\n",
        "    calculate feature importance for the Random Forest Classifier\n",
        "    :param X: features\n",
        "    :param y: labels\n",
        "    :param feature_name: the name of the features\n",
        "    \"\"\"\n",
        "    # define and fit classifier\n",
        "    rf_clf = RandomForestClassifier(n_estimators=100, max_features=16, max_depth=5,\n",
        "                                    random_state=rnd_seed)\n",
        "    rf_clf.fit(X, y)\n",
        "\n",
        "    # feature importance\n",
        "    importances = rf_clf.feature_importances_\n",
        "    std = np.std([tree.feature_importances_ for tree in rf_clf.estimators_], axis=0)\n",
        "    indices = np.argsort(importances)[::-1]\n",
        "\n",
        "    # Print the feature ranking\n",
        "    print(\"Feature ranking:\")\n",
        "    for f in range(len(feature_name)):\n",
        "        print(\"%d. feature %d (%s) (%f)\" % (f + 1, indices[f], feature_name[indices[f]],\n",
        "                                            importances[indices[f]]))\n",
        "\n",
        "    # Plot the impurity-based feature importances of the forest\n",
        "    plt.figure()\n",
        "    plt.title(\"Feature Importance\")\n",
        "    plt.bar(range(len(feature_name)), importances[indices], color=\"g\", yerr=std[indices], align=\"center\")\n",
        "    plt.xticks(range(len(feature_name)), indices)\n",
        "    plt.xlim([-1, len(feature_name)])\n",
        "    # plt.show()\n",
        "    plt.savefig(png_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnXvb-D5rejH"
      },
      "source": [
        "def read_emb_and_node_list(emb_file, node_file):\n",
        "    # read embedding\n",
        "    emb_df = pd.read_csv(emb_file, sep=' ', skiprows=1, header=None)\n",
        "    emb_df.columns = ['node'] + [f'emb_{i}' for i in range(emb_df.shape[1] - 1)]\n",
        "\n",
        "    # read node list\n",
        "    node_df = pd.read_csv(node_file)\n",
        "    node_df = node_df[['node', 'isp']]\n",
        "\n",
        "    # merge\n",
        "    merged_df = emb_df.merge(node_df, on='node', how='left')\n",
        "\n",
        "    return merged_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MY2k8UhOrgWw"
      },
      "source": [
        "def data_preproc_for_RiWalk_Binary_clf(emb_file, node_file):\n",
        "    \"\"\"\n",
        "    pre-process the RiWalk generated embedding for node classification\n",
        "    \"\"\"\n",
        "    # read and merge the data frames\n",
        "    merged_df = read_emb_and_node_list(emb_file, node_file)\n",
        "\n",
        "    # datasets for  BINARY classification\n",
        "    X = merged_df # only anchor nodes\n",
        "    y = X['isp'].tolist()\n",
        "    X = X.drop(['node', 'isp'], axis=1)\n",
        "    feature_names = X.columns\n",
        "    X = X.values.tolist()\n",
        "\n",
        "    # split the train and test set\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, rnd_seed)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test, feature_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIzx2yi0ricn"
      },
      "source": [
        "def prepare_data_for_concat_fe_emb(emb_file, fe_file):\n",
        "    \"\"\"\n",
        "    pre-process the data for the node classification of a new dataset consisting of the\n",
        "    engineered features and the embeddings\n",
        "    \"\"\"\n",
        "    # read embedding\n",
        "    emb_df = pd.read_csv(emb_file, sep=' ', skiprows=1, header=None)\n",
        "    emb_df.columns = ['node'] + [f'emb_{i}' for i in range(emb_df.shape[1] - 1)]\n",
        "\n",
        "    # read node list\n",
        "    node_df = pd.read_csv(fe_file)\n",
        "    # scale features\n",
        "    feature_col = [f for f in node_df.columns if f not in ['node', 'isp']]\n",
        "    scaler = StandardScaler()\n",
        "    node_df[feature_col] = scaler.fit_transform(node_df[feature_col])\n",
        "\n",
        "    # merge\n",
        "    merged_df = emb_df.merge(node_df, on='node', how='left')\n",
        "\n",
        "    # datasets for  BINARY classification\n",
        "    X = merged_df  # only anchor nodes\n",
        "    y = X['isp'].tolist()\n",
        "    X = X.drop(['node', 'isp'], axis=1)\n",
        "    X = X.values.tolist()\n",
        "\n",
        "    # split the train and test set\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, rnd_seed)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPIZNU2WrkSW"
      },
      "source": [
        "def plot_TSNE(values, labels, png_file):\n",
        "    \"\"\"\n",
        "    plot the embeddings as a TSNE graph\n",
        "    \"\"\"\n",
        "    print('\\tt-SNE starts.')\n",
        "    time_start = time.time()\n",
        "    tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
        "    tsne_results = tsne.fit_transform(values)\n",
        "    print('\\tt-SNE done! Time elapsed: {} seconds'.format(time.time() - time_start))\n",
        "\n",
        "    # plotting\n",
        "    p_data = {'tsne-2d-first': tsne_results[:, 0],\n",
        "              'tsne-2d-second': tsne_results[:, 1],\n",
        "              'label': labels,\n",
        "              }\n",
        "\n",
        "    plt.figure(figsize=(16, 10))\n",
        "    sns.scatterplot(\n",
        "        x=\"tsne-2d-first\", y=\"tsne-2d-second\",\n",
        "        hue=\"label\",\n",
        "        palette=sns.color_palette(\"hls\", len(set(labels))),\n",
        "        data=p_data,\n",
        "        legend=\"full\",\n",
        "        alpha=0.3\n",
        "    )\n",
        "    # plt.show()\n",
        "    plt.savefig(png_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrjavw4xrmjW"
      },
      "source": [
        "def EF_analysis_selected_nodes(output_path, graph, edges_filename, nodes_filename,\n",
        "                               features_filename, stats_file, feat_imp_filename,\n",
        "                               flag, binary, rnd_seed, exp_id, extra_analysis):\n",
        "    # print(\"\\tRead edge list and node list.\")\n",
        "    # start_time = time.time()\n",
        "    # edges_df = pd.read_csv(edges_filename)\n",
        "    nodes_df = pd.read_csv(nodes_filename)\n",
        "    # print(\"\\t\\tTime elapsed {} seconds.\".format(time.time() - start_time))\n",
        "\n",
        "    print(\"\\tRetrieve anchor nodes for classification.\")\n",
        "    start_time = time.time()\n",
        "    selected_node_list = nodes_df['node'].tolist()\n",
        "    print(\"\\t\\tTime elapsed {} seconds.\".format(time.time() - start_time))\n",
        "\n",
        "\n",
        "    print(\"\\tRead features for anchor nodes.\")\n",
        "    start_time = time.time()\n",
        "    all_node_features_df = pd.read_csv(features_filename)\n",
        "    features_df = all_node_features_df.loc[all_node_features_df['node'].isin(selected_node_list)]\n",
        "    print(\"\\t\\tTime elapsed {} seconds.\".format(time.time() - start_time))\n",
        "\n",
        "    # make ready for classification\n",
        "    # features_df = pd.read_csv(features_filename)\n",
        "    y = features_df['isp'].tolist()  # only anchor nodes where selected\n",
        "    X_orig = features_df.drop(['node', 'isp'], axis=1)\n",
        "    feature_names = X_orig.columns\n",
        "    X_orig = X_orig.values.tolist()\n",
        "\n",
        "    # split the train and test set\n",
        "    print(\"\\tTrain-Test split.\")\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_orig, y, rnd_seed)\n",
        "\n",
        "    # scale the features; note that it should be fitted on the train set ONLY\n",
        "    print('\\tScaling the features.')\n",
        "    min_max_scaler = MinMaxScaler()\n",
        "    min_max_scaler.fit(X_train)\n",
        "    X_train_scaled = min_max_scaler.transform(X_train)\n",
        "    X_test_scaled = min_max_scaler.transform(X_test)\n",
        "\n",
        "    # classification\n",
        "    print('\\tApplying classification.')\n",
        "    start_time = time.time()\n",
        "    rf_perf, rf_clf, lr_perf, lr_clf = rf_lr_classification(X_train_scaled, X_test_scaled, y_train,\n",
        "                                                            y_test, stats_file, flag, binary,\n",
        "                                                            exp_id, print_report=True)\n",
        "    print(\"\\t\\tTime elapsed {} seconds.\".format(time.time() - start_time))\n",
        "\n",
        "    # calculates and saves features importance\n",
        "    feature_imp_df = RF_sorted_feature_importance(rf_clf, feature_names)\n",
        "    feature_imp_df.to_csv(feat_imp_filename, index=False)\n",
        "\n",
        "    if extra_analysis:\n",
        "        # Feature importance\n",
        "        print(\"\\tInvestigate feature importance.\")\n",
        "        png_file = output_path + '/' + graph + '_' + flag + '_FE_feature_impo.png'\n",
        "        RF_feature_imp(X_train_scaled, y_train, feature_names, png_file)\n",
        "\n",
        "        # plot t-SNE graph\n",
        "        print(\"\\tt-SNE graph.\")\n",
        "        values = X_orig\n",
        "        groups = y\n",
        "        png_file = output_path + '/' + graph + '_' + flag + '_FE_tsne.png'\n",
        "        plot_TSNE(values, groups, png_file)\n",
        "\n",
        "    print(\"FE node classification finished.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hgnOcdcroao"
      },
      "source": [
        "def RiWalk_analysis_selected_nodes(output_path, graph, emb_filename, nodes_filename, stats_filename,\n",
        "                                   flag, binary, exp_id, extra_analysis):\n",
        "    # prepare the data\n",
        "    print(\"\\tPrepare data sets.\")\n",
        "    X_train, X_test, y_train, y_test, feature_names = data_preproc_for_RiWalk_Binary_clf(emb_filename,\n",
        "                                                                                         nodes_filename)\n",
        "    # classification\n",
        "    print('\\tApplying classification.')\n",
        "    start_time = time.time()\n",
        "    rf_lr_classification(X_train, X_test, y_train, y_test, stats_filename, flag,\n",
        "                         binary, exp_id, print_report=True)\n",
        "    print(\"\\tTime elapsed {} seconds.\".format(time.time() - start_time))\n",
        "\n",
        "    if extra_analysis:\n",
        "        # Feature importance\n",
        "        print(\"\\tInvestigate feature importance.\")\n",
        "        png_file = output_path + '/' + graph + '_' + flag + '_Ri_feature_impo.png'\n",
        "        RF_feature_imp(X_train, y_train, feature_names, png_file)\n",
        "\n",
        "        # plot t-SNE graph\n",
        "        print(\"\\tPlot t-SNE.\")\n",
        "        values = X_train + X_test\n",
        "        groups = y_train + y_test\n",
        "        # nodes_df = pd.read_csv(nodes_filename)\n",
        "        png_file = output_path + '/' + graph + flag + '_Ri_tsne.png'\n",
        "        plot_TSNE(values, groups, png_file)\n",
        "\n",
        "    print(\"RiWalk node classification finished.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpiERCnPrrT_"
      },
      "source": [
        "def nd_clf_fe_emb_combined(emb_file, fe_file, stats_file, flag, binary, exp_id):\n",
        "    \"\"\"\n",
        "    apply the node classification based on a new feature set constructed by combining the\n",
        "    engineered features and the (structural) embedding generated by an automatic method like node2vec\n",
        "    \"\"\"\n",
        "    print(\"\\tConcatenating embedding with engineered features for node classification.\")\n",
        "    # data preparation\n",
        "    X_train, X_test, y_train, y_test = prepare_data_for_concat_fe_emb(emb_file, fe_file)\n",
        "\n",
        "    # classification\n",
        "    print('\\tApplying classification.')\n",
        "    start_time = time.time()\n",
        "    rf_lr_classification(X_train, X_test, y_train, y_test, stats_file, flag,\n",
        "                         binary, exp_id, print_report=True)\n",
        "    print(\"\\tTime elapsed {} seconds.\".format(time.time() - start_time))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUAk2T2yZBbx",
        "outputId": "d36f562f-cbcc-476a-941d-cf30041aefdc"
      },
      "source": [
        "\"\"\"\n",
        "Node2Vec graph embedding method\n",
        "The source code is from the repository of the authors of the paper\n",
        "\"\"\"\n",
        "\n",
        "# Import Statements\n",
        "\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import gensim\n",
        "from joblib import Parallel, delayed\n",
        "from collections import defaultdict\n",
        "import os\n",
        "import random\n",
        "import pandas as pd\n",
        "import time\n",
        "import csv\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------\n",
        "# Class Definition\n",
        "\n",
        "\n",
        "class Node2Vec:\n",
        "    FIRST_TRAVEL_KEY = 'first_travel_key'\n",
        "    PROBABILITIES_KEY = 'probabilities'\n",
        "    NEIGHBORS_KEY = 'neighbors'\n",
        "    WEIGHT_KEY = 'weight'\n",
        "    NUM_WALKS_KEY = 'num_walks'\n",
        "    WALK_LENGTH_KEY = 'walk_length'\n",
        "    P_KEY = 'p'\n",
        "    Q_KEY = 'q'\n",
        "\n",
        "    def __init__(self, node_list_file_name, edge_filename, graph: nx.Graph, dimensions: int = 128, walk_length: int = 80,\n",
        "                 num_walks: int = 10,\n",
        "                 p: float = 1,\n",
        "                 q: float = 1, weight_key: str = 'weight', workers: int = 10,\n",
        "                 sampling_strategy: dict = None,\n",
        "                 quiet: bool = True, temp_folder: str = None):\n",
        "        \"\"\"\n",
        "        Initiates the Node2Vec object, precomputes walking probabilities and generates the walks.\n",
        "\n",
        "        :param graph: Input graph\n",
        "        :param dimensions: Embedding dimensions (default: 128)\n",
        "        :param walk_length: Number of nodes in each walk (default: 80)\n",
        "        :param num_walks: Number of walks per node (default: 10)\n",
        "        :param p: Return hyper parameter (default: 1)\n",
        "        :param q: Inout parameter (default: 1)\n",
        "        :param weight_key: On weighted graphs, this is the key for the weight attribute (default: 'weight')\n",
        "        :param workers: Number of workers for parallel execution (default: 1)\n",
        "        :param sampling_strategy: Node specific sampling strategies, supports setting node specific 'q', 'p', 'num_walks' and 'walk_length'.\n",
        "        Use these keys exactly. If not set, will use the global ones which were passed on the object initialization\n",
        "        :param temp_folder: Path to folder with enough space to hold the memory map of self.d_graph (for big graphs); to be passed joblib.Parallel.temp_folder\n",
        "        \"\"\"\n",
        "\n",
        "        self.graph = graph\n",
        "        self.dimensions = dimensions\n",
        "        self.walk_length = walk_length\n",
        "        self.num_walks = num_walks\n",
        "        self.p = p\n",
        "        self.q = q\n",
        "        self.weight_key = weight_key\n",
        "        self.workers = workers\n",
        "        self.quiet = quiet\n",
        "        self.d_graph = defaultdict(dict)\n",
        "\n",
        "        if sampling_strategy is None:\n",
        "            self.sampling_strategy = {}\n",
        "        else:\n",
        "            self.sampling_strategy = sampling_strategy\n",
        "\n",
        "        self.temp_folder, self.require = None, None\n",
        "        if temp_folder:\n",
        "            if not os.path.isdir(temp_folder):\n",
        "                raise NotADirectoryError(\"temp_folder does not exist or is not a directory. ({})\".format(temp_folder))\n",
        "\n",
        "            self.temp_folder = temp_folder\n",
        "            self.require = \"sharedmem\"\n",
        "\n",
        "        self._new_compute_prob(edge_filename, node_list_file_name) # for trans2vec\n",
        "        # self._precompute_probabilities() #if there then node2vec\n",
        "        self.walks = self._generate_walks()\n",
        "\n",
        "    def _precompute_probabilities(self):\n",
        "        \"\"\"\n",
        "        Precomputes transition probabilities for each node.\n",
        "        \"\"\"\n",
        "\n",
        "        d_graph = self.d_graph\n",
        "\n",
        "        # nodes_generator = self.graph.nodes() if self.quiet \\\n",
        "        #     else tqdm(self.graph.nodes(), desc='Computing transition probabilities')\n",
        "        nodes_generator = self.graph.nodes()\n",
        "\n",
        "        for source in nodes_generator:\n",
        "\n",
        "            # Init probabilities dict for first travel\n",
        "            if self.PROBABILITIES_KEY not in d_graph[source]:\n",
        "                d_graph[source][self.PROBABILITIES_KEY] = dict()\n",
        "\n",
        "            for current_node in self.graph.neighbors(source):\n",
        "\n",
        "                # Init probabilities dict\n",
        "                if self.PROBABILITIES_KEY not in d_graph[current_node]:\n",
        "                    d_graph[current_node][self.PROBABILITIES_KEY] = dict()\n",
        "\n",
        "                unnormalized_weights = list()\n",
        "                d_neighbors = list()\n",
        "\n",
        "                # Calculate unnormalized weights\n",
        "                for destination in self.graph.neighbors(current_node):\n",
        "\n",
        "                    p = self.sampling_strategy[current_node].get(self.P_KEY,\n",
        "                                                                 self.p) if current_node in self.sampling_strategy else self.p\n",
        "                    q = self.sampling_strategy[current_node].get(self.Q_KEY,\n",
        "                                                                 self.q) if current_node in self.sampling_strategy else self.q\n",
        "\n",
        "                    if destination == source:  # Backwards probability\n",
        "                        ss_weight = self.graph[current_node][destination].get(self.weight_key, 1) * 1 / p\n",
        "                    elif destination in self.graph[source]:  # If the neighbor is connected to the source\n",
        "                        ss_weight = self.graph[current_node][destination].get(self.weight_key, 1)\n",
        "                    else:\n",
        "                        ss_weight = self.graph[current_node][destination].get(self.weight_key, 1) * 1 / q\n",
        "\n",
        "                    # Assign the unnormalized sampling strategy weight, normalize during random walk\n",
        "                    unnormalized_weights.append(ss_weight)\n",
        "                    d_neighbors.append(destination)\n",
        "\n",
        "                # Normalize\n",
        "                unnormalized_weights = np.array(unnormalized_weights)\n",
        "                d_graph[current_node][self.PROBABILITIES_KEY][\n",
        "                    source] = unnormalized_weights / unnormalized_weights.sum()\n",
        "\n",
        "                # Save neighbors\n",
        "                d_graph[current_node][self.NEIGHBORS_KEY] = d_neighbors\n",
        "\n",
        "            # Calculate first_travel weights for source\n",
        "            first_travel_weights = []\n",
        "\n",
        "            for destination in self.graph.neighbors(source):\n",
        "                first_travel_weights.append(self.graph[source][destination].get(self.weight_key, 1))\n",
        "\n",
        "            first_travel_weights = np.array(first_travel_weights)\n",
        "            d_graph[source][self.FIRST_TRAVEL_KEY] = first_travel_weights / first_travel_weights.sum()\n",
        "\n",
        "    def _new_compute_prob(self, edge_filename, node_list_file_name):\n",
        "      data = pd.read_csv(edge_filename)\n",
        "      nodes_data = pd.read_csv(node_list_file_name)\n",
        "      d_graph = self.d_graph\n",
        "      nodes_generator = self.graph.nodes()\n",
        "      for source in nodes_generator:\n",
        "          df_source = data.loc[data['source'] == source]\n",
        "          unique_to = pd.unique(df_source['target']).tolist()\n",
        "          total = df_source['amount'].sum()\n",
        "          for destination in unique_to:\n",
        "              df_target = df_source.loc[data['target'] == destination]\n",
        "              sum = df_target['amount'].sum()\n",
        "              prob = sum / total\n",
        "              d_graph[source][destination] = prob\n",
        "              \n",
        "          df_node = nodes_data.loc[nodes_data['node'] == source]\n",
        "          is_p = df_node.iloc[0]['isp']  \n",
        "          d_graph[source][self.PROBABILITIES_KEY] = is_p\n",
        "          d_graph[source][self.FIRST_TRAVEL_KEY] = []\n",
        "\n",
        "    def _generate_walks(self) -> list:\n",
        "        \"\"\"\n",
        "        Generates the random walks which will be used as the skip-gram input.\n",
        "        :return: List of walks. Each walk is a list of nodes.\n",
        "        \"\"\"\n",
        "\n",
        "        flatten = lambda l: [item for sublist in l for item in sublist]\n",
        "\n",
        "        # Split num_walks for each worker\n",
        "        num_walks_lists = np.array_split(range(self.num_walks), self.workers)\n",
        "\n",
        "        walk_results = Parallel(n_jobs=self.workers, temp_folder=self.temp_folder, require=self.require)(\n",
        "            delayed(parallel_generate_walks)(self.d_graph,\n",
        "                                             self.walk_length,\n",
        "                                             len(num_walks),\n",
        "                                             idx,\n",
        "                                             self.sampling_strategy,\n",
        "                                             self.NUM_WALKS_KEY,\n",
        "                                             self.WALK_LENGTH_KEY,\n",
        "                                             self.NEIGHBORS_KEY,\n",
        "                                             self.PROBABILITIES_KEY,\n",
        "                                             self.FIRST_TRAVEL_KEY,\n",
        "                                             self.quiet) for\n",
        "            idx, num_walks\n",
        "            in enumerate(num_walks_lists, 1))\n",
        "\n",
        "        walks = flatten(walk_results)\n",
        "\n",
        "        return walks\n",
        "\n",
        "    def fit(self, **skip_gram_params) -> gensim.models.Word2Vec:\n",
        "        \"\"\"\n",
        "        Creates the embeddings using gensim's Word2Vec.\n",
        "        :param skip_gram_params: Parameteres for gensim.models.Word2Vec - do not supply 'size' it is taken from the Node2Vec 'dimensions' parameter\n",
        "        :type skip_gram_params: dict\n",
        "        :return: A gensim word2vec model\n",
        "        \"\"\"\n",
        "\n",
        "        if 'workers' not in skip_gram_params:\n",
        "            skip_gram_params['workers'] = self.workers\n",
        "\n",
        "        if 'size' not in skip_gram_params:\n",
        "            skip_gram_params['size'] = self.dimensions\n",
        "\n",
        "        return gensim.models.Word2Vec(self.walks, **skip_gram_params)\n",
        "\n",
        "\n",
        "def parallel_generate_walks(d_graph: dict, global_walk_length: int, num_walks: int, cpu_num: int,\n",
        "                            sampling_strategy: dict = None, num_walks_key: str = None,\n",
        "                            walk_length_key: str = None, neighbors_key: str = None,\n",
        "                            probabilities_key: str = None, first_travel_key: str = None,\n",
        "                            quiet: bool = False) -> list:\n",
        "    \"\"\"\n",
        "    Generates the random walks which will be used as the skip-gram input.\n",
        "    :return: List of walks. Each walk is a list of nodes.\n",
        "    \"\"\"\n",
        "\n",
        "    walks = list()\n",
        "\n",
        "\n",
        "    for n_walk in range(num_walks):\n",
        "\n",
        "        # Shuffle the nodes\n",
        "        shuffled_nodes = list(d_graph.keys())\n",
        "        random.shuffle(shuffled_nodes)\n",
        "\n",
        "        # Start a random walk from every node\n",
        "        for source in shuffled_nodes:\n",
        "\n",
        "            # Skip nodes with specific num_walks\n",
        "            if source in sampling_strategy and \\\n",
        "                    num_walks_key in sampling_strategy[source] and \\\n",
        "                    sampling_strategy[source][num_walks_key] <= n_walk:\n",
        "                continue\n",
        "\n",
        "            # Start walk\n",
        "            walk = [source]\n",
        "\n",
        "            # Calculate walk length\n",
        "            if source in sampling_strategy:\n",
        "                walk_length = sampling_strategy[source].get(walk_length_key, global_walk_length)\n",
        "            else:\n",
        "                walk_length = global_walk_length\n",
        "\n",
        "            # Perform walk\n",
        "            while len(walk) < walk_length:\n",
        "\n",
        "                walk_options = d_graph[walk[-1]].get(neighbors_key, None)\n",
        "\n",
        "                # Skip dead end nodes\n",
        "                if not walk_options:\n",
        "                    break\n",
        "\n",
        "                if len(walk) == 1:  # For the first step\n",
        "                    probabilities = d_graph[walk[-1]][first_travel_key]\n",
        "                    walk_to = np.random.choice(walk_options, size=1, p=probabilities)[0]\n",
        "                else:\n",
        "                    probabilities = d_graph[walk[-1]][probabilities_key][walk[-2]]\n",
        "                    walk_to = np.random.choice(walk_options, size=1, p=probabilities)[0]\n",
        "\n",
        "                walk.append(walk_to)\n",
        "\n",
        "            walk = list(map(str, walk))  # Convert all to strings\n",
        "\n",
        "            walks.append(walk)\n",
        "\n",
        "    return walks\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    instantiate a node2vec object\n",
        "    \"\"\"\n",
        "    print(\"Node2Vec main method.\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    iter_num = 5\n",
        "    num_walks = 20\n",
        "    dim = 64\n",
        "    walk_length = 5\n",
        "    workers = 10\n",
        "    window_size = 10\n",
        "    p = 1\n",
        "    q = 1\n",
        "    exp_id = '1;elliptic'\n",
        "\n",
        "    edge_list_file_name = \"/content/drive/My Drive/Baseline/Dataset/edgelist.edgelist\"\n",
        "    node_list_file_name = \"/content/drive/My Drive/Baseline/Dataset/nodeData.csv\"\n",
        "    edge_filename = \"/content/drive/My Drive/Baseline/Dataset/edgeData.csv\"\n",
        "    stats_file = \"/content/drive/My Drive/Baseline/Dataset/stats.csv\"\n",
        "    embeddings_filename = \"/content/drive/My Drive/Baseline/Dataset/embeddings.emb\"\n",
        "\n",
        "    nx_g = nx.from_pandas_edgelist(pd.read_csv(edge_list_file_name), source='source', target='target',\n",
        "                                   create_using=nx.DiGraph())\n",
        "    print(\"Graph info:\", nx.info(nx_g))\n",
        "\n",
        "    print(\"\\tInstantiate a node2vec object.\")\n",
        "    node2vec = Node2Vec(node_list_file_name, edge_filename, nx_g, dimensions=dim, walk_length=walk_length,\n",
        "                        num_walks=num_walks, workers=workers, p=p, q=q)\n",
        "    print(\"\\tFit node2vec.\")\n",
        "    model = node2vec.fit(window=window_size, sg=1, hs=0, min_count=1, iter=iter_num)\n",
        "\n",
        "    # read node list\n",
        "    print(\"\\tExtract embeddings and labels for the anchor nodes.\")\n",
        "    nodes_df = pd.read_csv(node_list_file_name)\n",
        "\n",
        "    # binary classification anchor nodes\n",
        "    anchor_nodes_df = nodes_df\n",
        "    node_list = [str(node_id) for node_id in anchor_nodes_df['node'].tolist()]\n",
        "    embeddings = [model.wv.get_vector(node) for node in node_list]\n",
        "    model.wv.save_word2vec_format(embeddings_filename)\n",
        "    labels = anchor_nodes_df['isp'].tolist()\n",
        "\n",
        "\n",
        "    # classification\n",
        "    print(\"\\tApply classification.\")\n",
        "    rnd_seed = 42\n",
        "    binary = True\n",
        "    X_train, X_test, y_train, y_test = train_test_split(embeddings, labels, rnd_seed)\n",
        "    rf_lr_classification(X_train, X_test, y_train, y_test, stats_file, 'n2v',\n",
        "                                            binary, exp_id, print_report=True)\n",
        "    print(\"Total elapsed time:\", str(time.time() - start_time))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Node2Vec main method.\n",
            "Graph info: DiGraph with 35417 nodes and 45377 edges\n",
            "\tInstantiate a node2vec object.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:162: RuntimeWarning: invalid value encountered in double_scalars\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done\n",
            "\tFit node2vec.\n",
            "\tExtract embeddings and labels for the anchor nodes.\n",
            "Tanay\n",
            "<class 'list'>\n",
            "Tanay\n",
            "\tApply classification.\n",
            "done 1\n",
            "done 2\n",
            "done 3\n",
            ">>> Binary Classification.\n",
            "\t*** RF - Training Set performance reports: ***\n",
            "\t\tPrecision: 0.000 \n",
            "\t\tRecall: 0.000 \n",
            "\t\tF1-Score: 0.000\n",
            "\t\tMicro-Average F1-Score: 0.000\n",
            "\t\tAccuracy: 0.967\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      1.00      0.98     27401\n",
            "           1       0.00      0.00      0.00       932\n",
            "\n",
            "    accuracy                           0.97     28333\n",
            "   macro avg       0.48      0.50      0.49     28333\n",
            "weighted avg       0.94      0.97      0.95     28333\n",
            "\n",
            ">>> Binary Classification.\n",
            "\t*** RF - Test Set performance reports: ***\n",
            "\t\tPrecision: 0.000 \n",
            "\t\tRecall: 0.000 \n",
            "\t\tF1-Score: 0.000\n",
            "\t\tMicro-Average F1-Score: 0.000\n",
            "\t\tAccuracy: 0.967\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      1.00      0.98      6851\n",
            "           1       0.00      0.00      0.00       233\n",
            "\n",
            "    accuracy                           0.97      7084\n",
            "   macro avg       0.48      0.50      0.49      7084\n",
            "weighted avg       0.94      0.97      0.95      7084\n",
            "\n",
            "done all\n",
            "{'index': '1', 'exp_id': 'elliptic', 'emb_method': 'n2v', 'classifier': 'RF', 'train_prec': 0.0, 'train_rec': 0.0, 'train_f1': 0.0, 'train_acc': 0.9671054953587689, 'train_auc': 0.7134372386709987, 'test_prec': 0.0, 'test_rec': 0.0, 'test_f1': 0.0, 'test_acc': 0.9671089779785432, 'test_auc': 0.5081623997749772}\n",
            "done 1\n",
            "done 2\n",
            "done 3\n",
            ">>> Binary Classification.\n",
            "\t*** LR - Training Set performance reports: ***\n",
            "\t\tPrecision: 0.000 \n",
            "\t\tRecall: 0.000 \n",
            "\t\tF1-Score: 0.000\n",
            "\t\tMicro-Average F1-Score: 0.000\n",
            "\t\tAccuracy: 0.967\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      1.00      0.98     27401\n",
            "           1       0.00      0.00      0.00       932\n",
            "\n",
            "    accuracy                           0.97     28333\n",
            "   macro avg       0.48      0.50      0.49     28333\n",
            "weighted avg       0.94      0.97      0.95     28333\n",
            "\n",
            ">>> Binary Classification.\n",
            "\t*** LR - Test Set performance reports: ***\n",
            "\t\tPrecision: 0.000 \n",
            "\t\tRecall: 0.000 \n",
            "\t\tF1-Score: 0.000\n",
            "\t\tMicro-Average F1-Score: 0.000\n",
            "\t\tAccuracy: 0.967\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      1.00      0.98      6851\n",
            "           1       0.00      0.00      0.00       233\n",
            "\n",
            "    accuracy                           0.97      7084\n",
            "   macro avg       0.48      0.50      0.49      7084\n",
            "weighted avg       0.94      0.97      0.95      7084\n",
            "\n",
            "done all\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'index': '1', 'exp_id': 'elliptic', 'emb_method': 'n2v', 'classifier': 'LR', 'train_prec': 0.0, 'train_rec': 0.0, 'train_f1': 0.0, 'train_acc': 0.9671054953587689, 'train_auc': 0.5, 'test_prec': 0.0, 'test_rec': 0.0, 'test_f1': 0.0, 'test_acc': 0.9671089779785432, 'test_auc': 0.5}\n",
            "Total elapsed time: 574.583459854126\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "flag = 'sp'\n",
        "binary = True\n",
        "clf_opt = 'fe'\n",
        "exp_id = '1;elliptic'\n",
        "emb_filename = '/content/drive/My Drive/Embeddings/embeddings1.emb'\n",
        "nodes_filename = \"/content/drive/My Drive/Initial CSV for FeatureEngineering/nodeData1.csv\"\n",
        "edges_filename = '/content/drive/My Drive/Riwalk_T2/edgelist1.edgelist'\n",
        "prod_data_dir = \"/content/drive/My Drive/SIGTRAN/1/\"\n",
        "graph_filename = 'graph_filename'\n",
        "stats_file = \"/content/drive/My Drive/SIGTRAN/stats.csv\"\n",
        "\n",
        "RiWalk_analysis_selected_nodes(prod_data_dir, graph_filename, emb_filename, nodes_filename, stats_file,flag, binary, exp_id, extra_analysis=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7fvIJ9dyJTQ",
        "outputId": "648428c8-16c0-496c-9809-09712779fdd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tPrepare data sets.\n",
            "\tApplying classification.\n",
            "done 1\n",
            "done 2\n",
            "done 3\n",
            "\t*** SVC - Training Set performance reports: ***\n",
            "\t\tPrecision: 0.905 \n",
            "\t\tRecall: 0.195 \n",
            "\t\tF1-Score: 0.321\n",
            "\t\tMicro-Average F1-Score: 0.321\n",
            "\t\tAccuracy: 0.974\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      1.00      0.99     28198\n",
            "           1       0.91      0.20      0.32       932\n",
            "\n",
            "    accuracy                           0.97     29130\n",
            "   macro avg       0.94      0.60      0.65     29130\n",
            "weighted avg       0.97      0.97      0.97     29130\n",
            "\n",
            "\t*** SVC - Test Set performance reports: ***\n",
            "\t\tPrecision: 0.622 \n",
            "\t\tRecall: 0.099 \n",
            "\t\tF1-Score: 0.170\n",
            "\t\tMicro-Average F1-Score: 0.170\n",
            "\t\tAccuracy: 0.969\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      1.00      0.98      7050\n",
            "           1       0.62      0.10      0.17       233\n",
            "\n",
            "    accuracy                           0.97      7283\n",
            "   macro avg       0.80      0.55      0.58      7283\n",
            "weighted avg       0.96      0.97      0.96      7283\n",
            "\n",
            "done all\n",
            "done 1\n",
            "done 2\n",
            "done 3\n",
            ">>> Multi-class Classification.\n",
            "\t*** SVM - Training Set performance reports: ***\n",
            "\t\tPrecision: 0.001 \n",
            "\t\tRecall: 0.012 \n",
            "\t\tF1-Score: 0.001\n",
            "\t\tMicro-Average F1-Score: 0.001\n",
            "\t\tAccuracy: 0.001\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.00      0.00      0.00         0\n",
            "           0       0.00      0.00      0.00     28198\n",
            "           1       0.00      0.04      0.00       932\n",
            "\n",
            "    accuracy                           0.00     29130\n",
            "   macro avg       0.00      0.01      0.00     29130\n",
            "weighted avg       0.00      0.00      0.00     29130\n",
            "\n",
            ">>> Multi-class Classification.\n",
            "\t*** SVM - Test Set performance reports: ***\n",
            "\t\tPrecision: 0.001 \n",
            "\t\tRecall: 0.017 \n",
            "\t\tF1-Score: 0.002\n",
            "\t\tMicro-Average F1-Score: 0.002\n",
            "\t\tAccuracy: 0.002\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.00      0.00      0.00         0\n",
            "           0       0.00      0.00      0.00      7050\n",
            "           1       0.00      0.05      0.01       233\n",
            "\n",
            "    accuracy                           0.00      7283\n",
            "   macro avg       0.00      0.02      0.00      7283\n",
            "weighted avg       0.00      0.00      0.00      7283\n",
            "\n",
            "done all\n",
            "\tTime elapsed 154.88899421691895 seconds.\n",
            "RiWalk node classification finished.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    }
  ]
}